{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535d4ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of data:393886\n"
     ]
    }
   ],
   "source": [
    "# Step1 读取数据\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Cx_330\\\\Desktop\\\\ypjbyc\\\\01_叶片结冰预测\\\\train\\\\15')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step1 读取数据\n",
    "data = pd.read_csv(\"15_data.csv\")\n",
    "total = len(data)\n",
    "print(\"sum of data:%d\" % total)\n",
    "des = data.describe()\n",
    "fail_data = pd.read_csv(\"15_failureInfo.csv\")\n",
    "normal_data = pd.read_csv(\"15_normalInfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2559447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete 0 / 393886\n",
      "complete 10000 / 393886\n",
      "complete 20000 / 393886\n",
      "complete 30000 / 393886\n",
      "complete 40000 / 393886\n",
      "complete 50000 / 393886\n",
      "complete 60000 / 393886\n",
      "complete 70000 / 393886\n",
      "complete 80000 / 393886\n",
      "complete 90000 / 393886\n",
      "complete 100000 / 393886\n",
      "complete 110000 / 393886\n",
      "complete 120000 / 393886\n",
      "complete 130000 / 393886\n",
      "complete 140000 / 393886\n",
      "complete 150000 / 393886\n",
      "complete 160000 / 393886\n",
      "complete 170000 / 393886\n",
      "complete 180000 / 393886\n",
      "complete 190000 / 393886\n",
      "complete 200000 / 393886\n",
      "complete 210000 / 393886\n",
      "complete 220000 / 393886\n",
      "complete 230000 / 393886\n",
      "complete 240000 / 393886\n",
      "complete 250000 / 393886\n",
      "complete 260000 / 393886\n",
      "complete 270000 / 393886\n",
      "complete 280000 / 393886\n",
      "complete 290000 / 393886\n",
      "complete 300000 / 393886\n",
      "complete 310000 / 393886\n",
      "complete 320000 / 393886\n",
      "complete 330000 / 393886\n",
      "complete 340000 / 393886\n",
      "complete 350000 / 393886\n",
      "complete 360000 / 393886\n",
      "complete 370000 / 393886\n",
      "complete 380000 / 393886\n",
      "complete 390000 / 393886\n"
     ]
    }
   ],
   "source": [
    "# 转化data时间列为datetime\n",
    "times = []\n",
    "for i in range(len(data)):\n",
    "    #dt = pd.to_datetime(data.ix[i][0])\n",
    "    dt = pd.to_datetime(data.loc[i][0])\n",
    "    times.append(dt)\n",
    "    if(i%10000==0):\n",
    "        print(\"complete %d / %d\" % (i,len(data)))\n",
    "times = pd.Series(times)\n",
    "data.time = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化normal_data & fail_data时间列为datetime\n",
    "\n",
    "def to_datetime(obj_pd): \n",
    "    Ser1 = obj_pd.iloc[:,0]\n",
    "    Ser2 = obj_pd.iloc[:,1]\n",
    "    for i in range(len(Ser1)):\n",
    "        Ser1[i] = pd.to_datetime(Ser1[i])\n",
    "        Ser2[i] = pd.to_datetime(Ser2[i])\n",
    "    obj_pd.iloc[:,0] = Ser1\n",
    "    obj_pd.iloc[:,1] = Ser2\n",
    "    return obj_pd\n",
    "        \n",
    "normal_data = to_datetime(normal_data)\n",
    "fail_data = to_datetime(fail_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472f5e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete 0 / 393886\n",
      "complete 100000 / 393886\n",
      "complete 200000 / 393886\n",
      "complete 300000 / 393886\n",
      "complete all\n"
     ]
    }
   ],
   "source": [
    "# 根据datetime创建labels列表\n",
    "\n",
    "labels = []\n",
    "for i in range(len(times)):\n",
    "    if(i%100000==0):\n",
    "        print(\"complete %d / %d\" % (i,len(times)))\n",
    "    flag = 0\n",
    "    for j in range(len(normal_data)):\n",
    "        if((times[i] >= normal_data.startTime[j]) and (times[i] <= normal_data.endTime[j])):\n",
    "            labels.append(0)\n",
    "            flag = 1\n",
    "            break\n",
    "    for j in range(len(fail_data)):\n",
    "        if(flag==1):\n",
    "            break\n",
    "        elif((times[i] >= fail_data.startTime[j]) and (times[i] <= fail_data.endTime[j])):\n",
    "            labels.append(1)\n",
    "            flag = 1\n",
    "            break\n",
    "    if(flag == 1):\n",
    "        continue\n",
    "    labels.append(-1)\n",
    "print(\"complete all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea521d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of invalid data : 19785 , 5.02 %\n",
      "sum of normal data : 350255 , 88.92 % \n",
      "sum of failure data : 23846 , 6.05 % \n"
     ]
    }
   ],
   "source": [
    "# print 数据信息\n",
    "def data_judge(labels,total):\n",
    "    sum_inv = 0\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i] == -1):\n",
    "            sum_inv = sum_inv + 1\n",
    "    print(\"sum of invalid data : %d , %.2f %%\" % (sum_inv,sum_inv/total*100))\n",
    "    \n",
    "    sum_nor = 0\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i] == 0):\n",
    "            sum_nor = sum_nor + 1\n",
    "    print(\"sum of normal data : %d , %.2f %% \" % (sum_nor,sum_nor/total*100))\n",
    "    \n",
    "    sum_fail = 0\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i] == 1):\n",
    "            sum_fail = sum_fail + 1\n",
    "    print(\"sum of failure data : %d , %.2f %% \" % (sum_fail,sum_fail/total*100))\n",
    "    \n",
    "data_judge(labels,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c422bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除无效数据\n",
    "y = labels\n",
    "indexes = []\n",
    "for i in range(len(y)):\n",
    "    if(y[i] == -1):\n",
    "        indexes.append(i)\n",
    "data = data.drop(indexes)\n",
    "data = data.drop('time',axis=1)\n",
    "for i in range(len(y)-1,-1,-1):\n",
    "    if(y[i]==-1):\n",
    "        y.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ba611f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>generator_speed</th>\n",
       "      <th>power</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_direction_mean</th>\n",
       "      <th>yaw_position</th>\n",
       "      <th>yaw_speed</th>\n",
       "      <th>pitch1_angle</th>\n",
       "      <th>pitch2_angle</th>\n",
       "      <th>pitch3_angle</th>\n",
       "      <th>...</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>environment_tmp</th>\n",
       "      <th>int_tmp</th>\n",
       "      <th>pitch1_ng5_tmp</th>\n",
       "      <th>pitch2_ng5_tmp</th>\n",
       "      <th>pitch3_ng5_tmp</th>\n",
       "      <th>pitch1_ng5_DC</th>\n",
       "      <th>pitch2_ng5_DC</th>\n",
       "      <th>pitch3_ng5_DC</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.859993</td>\n",
       "      <td>1.223595</td>\n",
       "      <td>2.515790</td>\n",
       "      <td>-2.072739</td>\n",
       "      <td>-2.073627</td>\n",
       "      <td>-0.655343</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.506667</td>\n",
       "      <td>0.551111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.403919</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.911625</td>\n",
       "      <td>1.293394</td>\n",
       "      <td>2.313551</td>\n",
       "      <td>-2.010591</td>\n",
       "      <td>-1.615140</td>\n",
       "      <td>-0.655343</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.209522</td>\n",
       "      <td>-0.421277</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.88</td>\n",
       "      <td>-2.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.635027</td>\n",
       "      <td>1.280099</td>\n",
       "      <td>2.507799</td>\n",
       "      <td>-2.053750</td>\n",
       "      <td>-0.282742</td>\n",
       "      <td>-0.649566</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.421277</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.786234</td>\n",
       "      <td>1.280099</td>\n",
       "      <td>2.349593</td>\n",
       "      <td>-2.007138</td>\n",
       "      <td>-2.234477</td>\n",
       "      <td>-0.655343</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.168889</td>\n",
       "      <td>0.137778</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.403919</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.786234</td>\n",
       "      <td>1.263480</td>\n",
       "      <td>2.321566</td>\n",
       "      <td>-2.264365</td>\n",
       "      <td>-1.428959</td>\n",
       "      <td>-0.637917</td>\n",
       "      <td>0.414524</td>\n",
       "      <td>0.182222</td>\n",
       "      <td>0.168889</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.209522</td>\n",
       "      <td>-0.403919</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393880</th>\n",
       "      <td>-0.474489</td>\n",
       "      <td>-1.123012</td>\n",
       "      <td>-0.863113</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>-0.009170</td>\n",
       "      <td>-0.684464</td>\n",
       "      <td>0.309873</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-1.604824</td>\n",
       "      <td>-1.665902</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.32</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393881</th>\n",
       "      <td>-0.113069</td>\n",
       "      <td>-1.179516</td>\n",
       "      <td>-0.883126</td>\n",
       "      <td>0.033416</td>\n",
       "      <td>0.826745</td>\n",
       "      <td>-0.742660</td>\n",
       "      <td>-0.527336</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-1.620604</td>\n",
       "      <td>-1.665902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.013846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393882</th>\n",
       "      <td>0.034450</td>\n",
       "      <td>-1.136307</td>\n",
       "      <td>-0.871127</td>\n",
       "      <td>0.145629</td>\n",
       "      <td>0.330262</td>\n",
       "      <td>-0.742660</td>\n",
       "      <td>-0.457568</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-1.620604</td>\n",
       "      <td>-1.665902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.013846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>1.84</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393883</th>\n",
       "      <td>0.292608</td>\n",
       "      <td>-1.129659</td>\n",
       "      <td>-0.877126</td>\n",
       "      <td>-0.085703</td>\n",
       "      <td>-1.540414</td>\n",
       "      <td>-0.731058</td>\n",
       "      <td>-0.073848</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.209522</td>\n",
       "      <td>-1.620604</td>\n",
       "      <td>-1.686936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.013846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393884</th>\n",
       "      <td>-0.035621</td>\n",
       "      <td>-1.159574</td>\n",
       "      <td>-0.877126</td>\n",
       "      <td>-0.622600</td>\n",
       "      <td>-1.442891</td>\n",
       "      <td>-0.731058</td>\n",
       "      <td>-0.073848</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-1.620604</td>\n",
       "      <td>-1.665902</td>\n",
       "      <td>1.013846</td>\n",
       "      <td>1.029231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374101 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wind_speed  generator_speed     power  wind_direction  \\\n",
       "0         1.859993         1.223595  2.515790       -2.072739   \n",
       "1         1.911625         1.293394  2.313551       -2.010591   \n",
       "2         1.635027         1.280099  2.507799       -2.053750   \n",
       "3         1.786234         1.280099  2.349593       -2.007138   \n",
       "4         1.786234         1.263480  2.321566       -2.264365   \n",
       "...            ...              ...       ...             ...   \n",
       "393880   -0.474489        -1.123012 -0.863113        0.042048   \n",
       "393881   -0.113069        -1.179516 -0.883126        0.033416   \n",
       "393882    0.034450        -1.136307 -0.871127        0.145629   \n",
       "393883    0.292608        -1.129659 -0.877126       -0.085703   \n",
       "393884   -0.035621        -1.159574 -0.877126       -0.622600   \n",
       "\n",
       "        wind_direction_mean  yaw_position  yaw_speed  pitch1_angle  \\\n",
       "0                 -2.073627     -0.655343   0.030804      0.555556   \n",
       "1                 -1.615140     -0.655343   0.030804      0.195556   \n",
       "2                 -0.282742     -0.649566   0.170338      0.964444   \n",
       "3                 -2.234477     -0.655343  -0.004080      0.168889   \n",
       "4                 -1.428959     -0.637917   0.414524      0.182222   \n",
       "...                     ...           ...        ...           ...   \n",
       "393880            -0.009170     -0.684464   0.309873      0.204444   \n",
       "393881             0.826745     -0.742660  -0.527336      0.204444   \n",
       "393882             0.330262     -0.742660  -0.457568      0.204444   \n",
       "393883            -1.540414     -0.731058  -0.073848      0.204444   \n",
       "393884            -1.442891     -0.731058  -0.073848      0.204444   \n",
       "\n",
       "        pitch2_angle  pitch3_angle  ...     acc_y  environment_tmp   int_tmp  \\\n",
       "0           0.506667      0.551111  ...  0.061109        -0.403919  0.014918   \n",
       "1           0.133333      0.191111  ... -1.209522        -0.421277 -0.002291   \n",
       "2           0.951111      0.960000  ...  0.061109        -0.421277 -0.002291   \n",
       "3           0.137778      0.177778  ...  0.061109        -0.403919 -0.002291   \n",
       "4           0.168889      0.204444  ... -1.209522        -0.403919  0.014918   \n",
       "...              ...           ...  ...       ...              ...       ...   \n",
       "393880      0.195556      0.204444  ...  0.061109        -1.604824 -1.665902   \n",
       "393881      0.195556      0.204444  ...  0.061109        -1.620604 -1.665902   \n",
       "393882      0.195556      0.204444  ...  0.061109        -1.620604 -1.665902   \n",
       "393883      0.195556      0.204444  ... -1.209522        -1.620604 -1.686936   \n",
       "393884      0.195556      0.204444  ...  0.061109        -1.620604 -1.665902   \n",
       "\n",
       "        pitch1_ng5_tmp  pitch2_ng5_tmp  pitch3_ng5_tmp  pitch1_ng5_DC  \\\n",
       "0             1.307692        1.123077        0.783077           1.36   \n",
       "1             1.307692        1.123077        0.783077           0.44   \n",
       "2             1.307692        1.123077        0.783077           1.76   \n",
       "3             1.307692        1.123077        0.783077           2.80   \n",
       "4             1.307692        1.123077        0.783077          -0.88   \n",
       "...                ...             ...             ...            ...   \n",
       "393880        0.984615        1.000000        1.000000           1.24   \n",
       "393881        1.000000        1.013846        1.000000          -0.52   \n",
       "393882        1.000000        1.013846        1.000000           0.32   \n",
       "393883        1.000000        1.013846        1.000000           1.20   \n",
       "393884        1.013846        1.029231        1.000000           0.76   \n",
       "\n",
       "        pitch2_ng5_DC  pitch3_ng5_DC  group  \n",
       "0                0.00           1.56      1  \n",
       "1                2.88          -2.60      1  \n",
       "2                0.60           2.56      1  \n",
       "3               -0.48           0.12      1  \n",
       "4                1.72           0.92      1  \n",
       "...               ...            ...    ...  \n",
       "393880           1.32          -0.40   3853  \n",
       "393881          -0.04          -1.48   3853  \n",
       "393882          -2.36           1.84   3853  \n",
       "393883           0.12           1.28   3853  \n",
       "393884          -0.40          -0.60   3853  \n",
       "\n",
       "[374101 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "717f41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e612c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 随机选择百分之10的数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=666, shuffle = True)# shuffle默认为True\n",
    "# 在选择的数据中，选择2/3作为训练集，1/3作为测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=666, shuffle = False)# shuffle默认为True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1038d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "x_train = scaler.transform(X_train)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19426ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import LSTM,Activation,Dense,Dropout,Input,Embedding,BatchNormalization,Add,concatenate,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50,return_sequences=True,input_shape=(1,26)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(LSTM(units=50,return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(units=256))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(units=128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=16))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=2,activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e2969a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,LearningRateScheduler\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"lstm.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1)\n",
    "                              #min_delta = 0.00001)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06ccd804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>generator_speed</th>\n",
       "      <th>power</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_direction_mean</th>\n",
       "      <th>yaw_position</th>\n",
       "      <th>yaw_speed</th>\n",
       "      <th>pitch1_angle</th>\n",
       "      <th>pitch2_angle</th>\n",
       "      <th>pitch3_angle</th>\n",
       "      <th>...</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>environment_tmp</th>\n",
       "      <th>int_tmp</th>\n",
       "      <th>pitch1_ng5_tmp</th>\n",
       "      <th>pitch2_ng5_tmp</th>\n",
       "      <th>pitch3_ng5_tmp</th>\n",
       "      <th>pitch1_ng5_DC</th>\n",
       "      <th>pitch2_ng5_DC</th>\n",
       "      <th>pitch3_ng5_DC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21704</th>\n",
       "      <td>-0.692079</td>\n",
       "      <td>-0.906964</td>\n",
       "      <td>-0.697198</td>\n",
       "      <td>-1.903557</td>\n",
       "      <td>-2.045763</td>\n",
       "      <td>-0.154786</td>\n",
       "      <td>0.135455</td>\n",
       "      <td>0.351111</td>\n",
       "      <td>0.337778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>0.714927</td>\n",
       "      <td>0.990138</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137032</th>\n",
       "      <td>-0.695767</td>\n",
       "      <td>-0.837165</td>\n",
       "      <td>-0.665163</td>\n",
       "      <td>-0.525924</td>\n",
       "      <td>-2.224345</td>\n",
       "      <td>-0.405088</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.328889</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.351111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.231910</td>\n",
       "      <td>-0.040535</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.046154</td>\n",
       "      <td>0.629231</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327740</th>\n",
       "      <td>-0.703143</td>\n",
       "      <td>-0.700889</td>\n",
       "      <td>-0.651149</td>\n",
       "      <td>0.347613</td>\n",
       "      <td>1.536006</td>\n",
       "      <td>-0.579674</td>\n",
       "      <td>-0.108731</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-1.447018</td>\n",
       "      <td>-1.570292</td>\n",
       "      <td>1.090769</td>\n",
       "      <td>1.046154</td>\n",
       "      <td>0.706154</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223176</th>\n",
       "      <td>0.827363</td>\n",
       "      <td>1.243537</td>\n",
       "      <td>1.428437</td>\n",
       "      <td>-1.238909</td>\n",
       "      <td>-0.244746</td>\n",
       "      <td>1.201421</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.297778</td>\n",
       "      <td>0.231111</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.023986</td>\n",
       "      <td>-2.480153</td>\n",
       "      <td>-0.877337</td>\n",
       "      <td>-1.187853</td>\n",
       "      <td>0.936923</td>\n",
       "      <td>0.583077</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86442</th>\n",
       "      <td>0.757291</td>\n",
       "      <td>0.282958</td>\n",
       "      <td>-0.112464</td>\n",
       "      <td>1.381700</td>\n",
       "      <td>1.989426</td>\n",
       "      <td>-0.370142</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.337778</td>\n",
       "      <td>...</td>\n",
       "      <td>1.225767</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>0.209947</td>\n",
       "      <td>0.454723</td>\n",
       "      <td>0.583077</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>1.323077</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356701</th>\n",
       "      <td>-0.293779</td>\n",
       "      <td>-0.428337</td>\n",
       "      <td>-0.522987</td>\n",
       "      <td>0.646272</td>\n",
       "      <td>0.746953</td>\n",
       "      <td>-0.626269</td>\n",
       "      <td>0.379641</td>\n",
       "      <td>0.208889</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.023986</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.798434</td>\n",
       "      <td>-0.671560</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.215385</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154418</th>\n",
       "      <td>-0.489241</td>\n",
       "      <td>-1.152926</td>\n",
       "      <td>-1.039339</td>\n",
       "      <td>-3.626461</td>\n",
       "      <td>-0.362534</td>\n",
       "      <td>-0.684464</td>\n",
       "      <td>-0.038964</td>\n",
       "      <td>0.324444</td>\n",
       "      <td>0.248889</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.973599</td>\n",
       "      <td>-1.017667</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255130</th>\n",
       "      <td>-1.090380</td>\n",
       "      <td>-1.166221</td>\n",
       "      <td>-0.923175</td>\n",
       "      <td>1.228055</td>\n",
       "      <td>0.613966</td>\n",
       "      <td>0.933693</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>0.273070</td>\n",
       "      <td>-0.422974</td>\n",
       "      <td>1.090769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86833</th>\n",
       "      <td>-0.341723</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>-0.276669</td>\n",
       "      <td>0.483995</td>\n",
       "      <td>-1.237712</td>\n",
       "      <td>-0.451635</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.337778</td>\n",
       "      <td>...</td>\n",
       "      <td>1.225767</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>0.227306</td>\n",
       "      <td>0.494880</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.290769</td>\n",
       "      <td>0.644615</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11154</th>\n",
       "      <td>-1.492368</td>\n",
       "      <td>-1.129659</td>\n",
       "      <td>-0.993267</td>\n",
       "      <td>-0.396447</td>\n",
       "      <td>-0.674103</td>\n",
       "      <td>-0.346845</td>\n",
       "      <td>-0.038964</td>\n",
       "      <td>0.351111</td>\n",
       "      <td>0.337778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.023986</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.167209</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.246154</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25064 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wind_speed  generator_speed     power  wind_direction  \\\n",
       "21704    -0.692079        -0.906964 -0.697198       -1.903557   \n",
       "137032   -0.695767        -0.837165 -0.665163       -0.525924   \n",
       "327740   -0.703143        -0.700889 -0.651149        0.347613   \n",
       "223176    0.827363         1.243537  1.428437       -1.238909   \n",
       "86442     0.757291         0.282958 -0.112464        1.381700   \n",
       "...            ...              ...       ...             ...   \n",
       "356701   -0.293779        -0.428337 -0.522987        0.646272   \n",
       "154418   -0.489241        -1.152926 -1.039339       -3.626461   \n",
       "255130   -1.090380        -1.166221 -0.923175        1.228055   \n",
       "86833    -0.341723         0.036996 -0.276669        0.483995   \n",
       "11154    -1.492368        -1.129659 -0.993267       -0.396447   \n",
       "\n",
       "        wind_direction_mean  yaw_position  yaw_speed  pitch1_angle  \\\n",
       "21704             -2.045763     -0.154786   0.135455      0.351111   \n",
       "137032            -2.224345     -0.405088  -0.004080      0.328889   \n",
       "327740             1.536006     -0.579674  -0.108731      0.217778   \n",
       "223176            -0.244746      1.201421  -0.004080      0.297778   \n",
       "86442              1.989426     -0.370142  -0.004080      0.355556   \n",
       "...                     ...           ...        ...           ...   \n",
       "356701             0.746953     -0.626269   0.379641      0.208889   \n",
       "154418            -0.362534     -0.684464  -0.038964      0.324444   \n",
       "255130             0.613966      0.933693   0.030804      0.191111   \n",
       "86833             -1.237712     -0.451635  -0.004080      0.355556   \n",
       "11154             -0.674103     -0.346845  -0.038964      0.351111   \n",
       "\n",
       "        pitch2_angle  pitch3_angle  ...     acc_x     acc_y  environment_tmp  \\\n",
       "21704       0.337778      0.333333  ...  0.100890  0.061109         0.714927   \n",
       "137032      0.355556      0.351111  ...  0.100890  0.061109        -0.231910   \n",
       "327740      0.204444      0.222222  ...  0.100890  0.061109        -1.447018   \n",
       "223176      0.231111      0.204444  ... -1.023986 -2.480153        -0.877337   \n",
       "86442       0.360000      0.337778  ...  1.225767  0.061109         0.209947   \n",
       "...              ...           ...  ...       ...       ...              ...   \n",
       "356701      0.204444      0.204444  ... -1.023986  0.061109        -0.798434   \n",
       "154418      0.248889      0.253333  ...  0.100890  0.061109        -0.973599   \n",
       "255130      0.177778      0.217778  ...  0.100890  0.061109         0.273070   \n",
       "86833       0.360000      0.337778  ...  1.225767  0.061109         0.227306   \n",
       "11154       0.337778      0.333333  ... -1.023986  0.061109        -0.167209   \n",
       "\n",
       "         int_tmp  pitch1_ng5_tmp  pitch2_ng5_tmp  pitch3_ng5_tmp  \\\n",
       "21704   0.990138        0.815385        1.153846        0.661538   \n",
       "137032 -0.040535        0.923077        1.046154        0.629231   \n",
       "327740 -1.570292        1.090769        1.046154        0.706154   \n",
       "223176 -1.187853        0.936923        0.583077        1.123077   \n",
       "86442   0.454723        0.583077        0.753846        1.323077   \n",
       "...          ...             ...             ...             ...   \n",
       "356701 -0.671560        0.846154        1.215385        1.153846   \n",
       "154418 -1.017667        0.984615        1.076923        1.000000   \n",
       "255130 -0.422974        1.090769        0.846154        0.600000   \n",
       "86833   0.494880        0.846154        1.290769        0.644615   \n",
       "11154   0.055075        0.769231        1.246154        0.815385   \n",
       "\n",
       "        pitch1_ng5_DC  pitch2_ng5_DC  pitch3_ng5_DC  \n",
       "21704            0.40          -0.76           0.04  \n",
       "137032           2.24          -0.52          -0.52  \n",
       "327740          -0.80           0.68          -0.68  \n",
       "223176           0.68          -2.40           2.08  \n",
       "86442           -0.68          -0.20           0.40  \n",
       "...               ...            ...            ...  \n",
       "356701          -0.52          -0.68           1.28  \n",
       "154418          -0.48          -0.48           0.52  \n",
       "255130           0.36           1.88           0.60  \n",
       "86833            0.40           1.36          -1.88  \n",
       "11154           -0.76           1.64           1.72  \n",
       "\n",
       "[25064 rows x 26 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8bbaffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6cbfdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25064, 26)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "873a7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "XX_train = x_train.reshape((-1,1,26))\n",
    "XX_test = x_test.reshape((-1,1,26))\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3df653f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5001619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25064, 1, 26)\n",
      "(25064, 2)\n",
      "(12346, 1, 26)\n",
      "(12346, 2)\n"
     ]
    }
   ],
   "source": [
    "print(XX_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(XX_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae801cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Cx_330\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cx_330\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9466\n",
      "Epoch 1: val_loss improved from inf to 0.13548, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 37s 10ms/step - loss: 0.1608 - accuracy: 0.9465 - val_loss: 0.1355 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "   6/3133 [..............................] - ETA: 32s - loss: 0.0878 - accuracy: 0.9792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cx_330\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3133/3133 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9520\n",
      "Epoch 2: val_loss improved from 0.13548 to 0.12566, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 30s 9ms/step - loss: 0.1353 - accuracy: 0.9520 - val_loss: 0.1257 - val_accuracy: 0.9534 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9537\n",
      "Epoch 3: val_loss improved from 0.12566 to 0.11683, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 451s 144ms/step - loss: 0.1246 - accuracy: 0.9538 - val_loss: 0.1168 - val_accuracy: 0.9550 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9551\n",
      "Epoch 4: val_loss improved from 0.11683 to 0.11456, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 29s 9ms/step - loss: 0.1198 - accuracy: 0.9551 - val_loss: 0.1146 - val_accuracy: 0.9547 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9567\n",
      "Epoch 5: val_loss did not improve from 0.11456\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.1126 - accuracy: 0.9567 - val_loss: 0.1200 - val_accuracy: 0.9568 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3126/3133 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9585\n",
      "Epoch 6: val_loss did not improve from 0.11456\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.1077 - accuracy: 0.9585 - val_loss: 0.1146 - val_accuracy: 0.9554 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3125/3133 [============================>.] - ETA: 0s - loss: 0.1021 - accuracy: 0.9611\n",
      "Epoch 7: val_loss improved from 0.11456 to 0.10820, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.1020 - accuracy: 0.9611 - val_loss: 0.1082 - val_accuracy: 0.9610 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9624\n",
      "Epoch 8: val_loss improved from 0.10820 to 0.09872, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0989 - accuracy: 0.9625 - val_loss: 0.0987 - val_accuracy: 0.9636 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9649\n",
      "Epoch 9: val_loss improved from 0.09872 to 0.09343, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0949 - accuracy: 0.9648 - val_loss: 0.0934 - val_accuracy: 0.9640 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9669\n",
      "Epoch 10: val_loss improved from 0.09343 to 0.09121, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0900 - accuracy: 0.9669 - val_loss: 0.0912 - val_accuracy: 0.9665 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history_fit = model.fit(x=XX_train, \n",
    "                        y=Y_train, \n",
    "                        batch_size=8, \n",
    "                        epochs=10, \n",
    "                        verbose=1, \n",
    "                        validation_data=(XX_test, Y_test),\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a5b606b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9678\n",
      "Epoch 1: val_loss improved from 0.09121 to 0.08555, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 27s 8ms/step - loss: 0.0853 - accuracy: 0.9678 - val_loss: 0.0856 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9697\n",
      "Epoch 2: val_loss improved from 0.08555 to 0.07932, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0812 - accuracy: 0.9697 - val_loss: 0.0793 - val_accuracy: 0.9691 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9719\n",
      "Epoch 3: val_loss did not improve from 0.07932\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0767 - accuracy: 0.9719 - val_loss: 0.0903 - val_accuracy: 0.9636 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9707\n",
      "Epoch 4: val_loss improved from 0.07932 to 0.07427, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0723 - accuracy: 0.9707 - val_loss: 0.0743 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9728\n",
      "Epoch 5: val_loss improved from 0.07427 to 0.07138, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.0708 - accuracy: 0.9727 - val_loss: 0.0714 - val_accuracy: 0.9722 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9737\n",
      "Epoch 6: val_loss improved from 0.07138 to 0.06864, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0662 - accuracy: 0.9737 - val_loss: 0.0686 - val_accuracy: 0.9738 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9734\n",
      "Epoch 7: val_loss improved from 0.06864 to 0.06550, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0659 - accuracy: 0.9734 - val_loss: 0.0655 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9770\n",
      "Epoch 8: val_loss did not improve from 0.06550\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0597 - accuracy: 0.9770 - val_loss: 0.0773 - val_accuracy: 0.9731 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3126/3133 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9762\n",
      "Epoch 9: val_loss improved from 0.06550 to 0.05984, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.0602 - accuracy: 0.9763 - val_loss: 0.0598 - val_accuracy: 0.9777 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3126/3133 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9778\n",
      "Epoch 10: val_loss did not improve from 0.05984\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 0.0574 - accuracy: 0.9779 - val_loss: 0.0626 - val_accuracy: 0.9757 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9782\n",
      "Epoch 11: val_loss improved from 0.05984 to 0.05635, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.0551 - accuracy: 0.9783 - val_loss: 0.0563 - val_accuracy: 0.9774 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9803\n",
      "Epoch 12: val_loss did not improve from 0.05635\n",
      "3133/3133 [==============================] - 21s 7ms/step - loss: 0.0513 - accuracy: 0.9803 - val_loss: 0.0584 - val_accuracy: 0.9774 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3125/3133 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 0.9796\n",
      "Epoch 13: val_loss improved from 0.05635 to 0.05588, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.0507 - accuracy: 0.9796 - val_loss: 0.0559 - val_accuracy: 0.9784 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9800\n",
      "Epoch 14: val_loss did not improve from 0.05588\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.0514 - accuracy: 0.9801 - val_loss: 0.0569 - val_accuracy: 0.9808 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 0.9804\n",
      "Epoch 15: val_loss did not improve from 0.05588\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 0.0495 - accuracy: 0.9804 - val_loss: 0.0619 - val_accuracy: 0.9785 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.0486 - accuracy: 0.9808\n",
      "Epoch 16: val_loss improved from 0.05588 to 0.05179, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 0.0485 - accuracy: 0.9808 - val_loss: 0.0518 - val_accuracy: 0.9799 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9818\n",
      "Epoch 17: val_loss did not improve from 0.05179\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0458 - accuracy: 0.9818 - val_loss: 0.0555 - val_accuracy: 0.9789 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.0447 - accuracy: 0.9828\n",
      "Epoch 18: val_loss did not improve from 0.05179\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.0448 - accuracy: 0.9828 - val_loss: 0.0524 - val_accuracy: 0.9781 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9836\n",
      "Epoch 19: val_loss did not improve from 0.05179\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0424 - accuracy: 0.9836 - val_loss: 0.0520 - val_accuracy: 0.9798 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9849\n",
      "Epoch 20: val_loss improved from 0.05179 to 0.04999, saving model to lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.0362 - accuracy: 0.9849 - val_loss: 0.0500 - val_accuracy: 0.9832 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history_fit = model.fit(x=XX_train, \n",
    "                        y=Y_train, \n",
    "                        batch_size=8, \n",
    "                        epochs=20, \n",
    "                        verbose=2, \n",
    "                        validation_data=(XX_test, Y_test),\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bff31b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 1: val_loss improved from 0.04999 to 0.04830, saving model to lstm.h5\n",
      "3133/3133 - 24s - loss: 0.0340 - accuracy: 0.9868 - val_loss: 0.0483 - val_accuracy: 0.9848 - lr: 2.0000e-04 - 24s/epoch - 8ms/step\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: val_loss improved from 0.04830 to 0.04703, saving model to lstm.h5\n",
      "3133/3133 - 23s - loss: 0.0303 - accuracy: 0.9878 - val_loss: 0.0470 - val_accuracy: 0.9847 - lr: 2.0000e-04 - 23s/epoch - 7ms/step\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_loss improved from 0.04703 to 0.04618, saving model to lstm.h5\n",
      "3133/3133 - 24s - loss: 0.0304 - accuracy: 0.9875 - val_loss: 0.0462 - val_accuracy: 0.9847 - lr: 2.0000e-04 - 24s/epoch - 8ms/step\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.04618\n",
      "3133/3133 - 25s - loss: 0.0295 - accuracy: 0.9877 - val_loss: 0.0487 - val_accuracy: 0.9870 - lr: 2.0000e-04 - 25s/epoch - 8ms/step\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.04618\n",
      "3133/3133 - 24s - loss: 0.0295 - accuracy: 0.9883 - val_loss: 0.0533 - val_accuracy: 0.9855 - lr: 2.0000e-04 - 24s/epoch - 8ms/step\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.04618\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3133/3133 - 24s - loss: 0.0285 - accuracy: 0.9885 - val_loss: 0.0527 - val_accuracy: 0.9866 - lr: 2.0000e-04 - 24s/epoch - 8ms/step\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.04618\n",
      "3133/3133 - 24s - loss: 0.0267 - accuracy: 0.9898 - val_loss: 0.0477 - val_accuracy: 0.9868 - lr: 4.0000e-05 - 24s/epoch - 8ms/step\n",
      "Epoch 8/20\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.04618\n",
      "3133/3133 - 24s - loss: 0.0284 - accuracy: 0.9886 - val_loss: 0.0472 - val_accuracy: 0.9868 - lr: 4.0000e-05 - 24s/epoch - 8ms/step\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "history_fit = model.fit(x=XX_train, \n",
    "                        y=Y_train, \n",
    "                        batch_size=8, \n",
    "                        epochs=20, \n",
    "                        verbose=2, \n",
    "                        validation_data=(XX_test, Y_test),\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f9e29f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False,\n",
    "                         title='Confusion matrix',\n",
    "                         cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Normalization can be applied by setting `normalize = True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title, fontsize=17)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=14)\n",
    "    plt.yticks(tick_marks, classes, fontsize=14)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i,j], fontsize=15,\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=15)\n",
    "    plt.xlabel('Predicted label', fontsize=15)\n",
    "    #plt.savefig(\"matrix.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93aa5aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 2s 4ms/step\n",
      "[[0.98763736 0.06446991]\n",
      " [0.01236264 0.93553009]]\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_prediction = model.predict(XX_test)\n",
    "y_prediction = np.argmax (y_prediction, axis = 1)\n",
    "yy_test=np.argmax(Y_test, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(yy_test, y_prediction , normalize='pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "558eecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[11504    45]\n",
      " [  144   653]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAHvCAYAAAC7er6FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAByQElEQVR4nO3deXhM1+MG8PdmGxLJZJNECCFiDyKIoBKNtY2ldtEUVfxKqaKW2mKNnapWbRU7VaWtliaUVCyxxr4LYglBdpH1/P7Q3G9GEpKZLCbzfvrM88i959x77mSavDnn3HMlIYQAERERkY7QK+kGEBERERUnhh8iIiLSKQw/REREpFMYfoiIiEinMPwQERGRTmH4ISIiIp3C8ENEREQ6heGHiIiIdArDDxEREekUhh8iLXXjxg306dMHdnZ20NfXhyRJGDBgQLG3486dO5AkCZIkFfu56X8cHR0hSRIOHTpU0k0heucx/BD9Jzo6GgEBAXj//fdhb28PhUIBU1NT1KxZE35+fti1axfS0tJKupkAgOfPn+O9997D9u3bkZaWhsaNG6NFixaoUaNGSTftnefv7y+HNUmScOPGjTeWX7VqlUr50NDQQm2Lv78/YmNjC+2YRPR2BiXdAKJ3wfLlyzFhwgQkJSUBAGxtbeHi4oK0tDTcu3cPmzZtwqZNm+Dk5IS///4bTk5OJdrerVu34vHjx2jYsCGOHDkCY2PjEmuLoaEhatasWWLn11RgYCBmz56d5/6ffvqpyM49ffp0AMCAAQNgbm6u0bGcnJxQpkyZEv0sEGkL9vyQzpswYQJGjBiBpKQk9OnTBxcuXEBUVBROnTqFc+fO4dmzZwgNDUW3bt1w+/ZtREZGlnSTcenSJQBA27ZtS/yXXcWKFXH16lVcvXq1RNtRUDVr1oSenh7Wr1+PzMzMXMtcuXIFYWFhqF27djG3ruAOHDiAq1evomnTpiXdFKJ3HsMP6bTffvsN8+bNA/Dqr/CtW7eiXr16KmX09PTQokUL7Ny5E7///juUSmVJNFVFcnIyAJR48NFmlSpVQps2bfDgwQMEBQXlWmbt2rUAgE8//bQ4m0ZERYzhh3SWEAKTJk0CALRs2RJTpkx5ax0fHx+4urrm2H7mzBl8/PHHcHBwgEKhgKWlJby8vLBu3TpkZGTkeqysOSR37tzBpUuX0LdvX9jZ2UGhUMDJyQnjx49HYmKiSp0BAwZAkiQEBgYCeBXYss9HyeLl5aVS7m3nf/192bRpE9q0aQNra2sYGhrC2toaderUwccff4xff/1VpXx+Jjzv3LkTHTp0QPny5WFkZIQKFSqgW7dueU7OPXToECRJgqOjIwBgz5498Pb2hoWFBYyNjeHq6ooff/wxz/Pl18CBAwHkPrSVnp6OTZs2wcDAAH5+fnkeIzU1Fbt27cJnn32G+vXrw8rKCgqFAg4ODujTpw+OHz+eo07WvKMsVatWVfk++vv7y/uyfy+joqLwxRdfoFq1alAoFGjYsKFcLrcJzw8fPkT58uUhSRLmzJmTa/u///57SJIEMzMz3Lp1K8/rJCpVBJGOOnXqlAAgAIgdO3aofZwffvhB6OnpCQDCzMxMNG7cWFSpUkU+drt27cSLFy9y1Mvav2rVKlGmTBlhbGws3NzcROXKleV9LVq0EGlpaXKd2bNnixYtWggbGxsBQDg4OIgWLVrIryyenp4CgFi3bl2e7c46R0REhMr2gQMHyvsqVKggGjduLGrXri3MzMwEAOHu7q5SPiIiQi7/uoyMDNGvXz+V4zVp0kRYWVnJ2yZPnpyj3sGDBwUAUaVKFTFz5kwBQJQvX140btxYpe748ePzvL68TJs2TQAQ3t7e4uXLl8LCwkIoFArx7NkzlXK7du0SAETnzp1V3q/Dhw+rlLtw4YIAIPT09IStra1o2LChqF+/vjA3N5e3r127VqXO2rVrRYsWLeRjNm7cWOX7mL181vdy8uTJonz58kJfX1/Uq1dPuLq6qnwvsj5zBw8eVDnXX3/9JSRJEgYGBiI0NFRlX3h4uFAoFAKA2LJlS4HfSyJtxfBDOmvJkiXyL5+nT5+qdYzDhw/LwWfcuHHi5cuX8r69e/fKgWHYsGE56mad29DQUIwePVokJSXJ+4KCgoSxsbEAIAIDA3PU7d+/vwAgpk2blmu71A0/4eHhAoAwNTUV+/fvz1Hn5MmTYtWqVSrb3hR+Zs+eLQCIMmXKiK1bt8rb09PTxdy5c+V6P//8s0q9rPBjaGgoypQpI9avXy8yMzOFEEJkZmbKx9XT0xO3b9/O8xpzkz38CCHE8OHDBQDx3XffqZTr1KmTACB2794thMg7/ERFRYkNGzbk+AylpaWJLVu2CGNjY6FQKMT9+/dztCWvAJpd1vdSX19feHt7qxwne6jOK/wIIcTXX38th+WskJeQkCBq1KghAIhBgwbleX6i0ojhh3TW6NGjBQChVCrVPkbbtm1VfpG+buXKlfIv8dd/+WX94vPy8sq17ogRIwQA0a1btxz7iir8bN26VQAQXbt2zbPe6/IKP0lJSXLvx8yZM3Ot27dvXwFA1KlTR2V7VvgBIPz9/XOt6+LiIgCIZcuW5butQuQMP6dPnxYAhKurq1zm0aNHwsDAQNja2so9b3mFn7f55ptvBAAxf/78HPsKEn7Kly8vYmNj8yz3pvCTmpoqmjVrptKT9fHHHwsAonbt2irBm0gXcM4P6az4+HgAQLly5dSq/+LFCxw8eBAAMHbs2FzLDBw4EOXLl0daWlqek2pHjBiR63YPDw8AeOs6NIWpSpUqAIDjx4/j5s2bGh0rNDQUsbGxMDIyyvMax48fDwC4fPkyIiIici1T1O9Po0aN0KBBA5w9exbnzp0DAGzYsAHp6enw8/ODgUH+VgQJCQnBuHHj0KVLF3h5eaFly5Zo2bIlduzYAQA4ffq0Ru3s0aOH2pPtDQ0NsXXrVpibm+P333/Hhx9+iE2bNqFMmTLYvn07J86TzuE6P6SzzMzMACDHpOL8unnzJtLT0wEALi4uuZYxNDRE7dq1ER0dneet4HmtkWNra6tR+9TRrFkzeHp6IiQkBDVr1kSLFi3QqlUrNGvWDO+9916BfvlmXW/lypXzrFe3bl3o6+sjIyMDV69eRdWqVVX2W1tbw9LSMte6hfn+DBw4EKNGjcJPP/2Eb7/9FuvWrZO3v01SUhJ69eqFv/76643lnj17plEb69atq1F9R0dHrF27Ft27d5fbunTp0jw/u0SlGXt+SGdVqlQJABAXF6fWL6asniPgf7+Ic1OhQoUc5bMzMTHJdbue3qv/PfNag6YoSJKEPXv2YMqUKXBwcMDhw4cxe/ZsdOrUCeXLl0evXr1y3B2Wl6zrtbOzy7OMgYEBrK2tVcpnl9d7AxTu+/Pxxx/DyMgImzdvxsGDB3H16lW4u7ujTp06b607btw4/PXXX7CwsMCKFStw7do1JCUlITMzE0II+XZ5TVcHf9N7kV8eHh4wNTUF8Cr89+3bV+NjEmkjhh/SWa1atZL//c8//xS4flbPEQA8fvw4z3KPHj3KUb6oZd1GLYTIdX9qamqedcuVK4cZM2bgzp07iIiIwObNmzFo0CCULVsWO3bsgLe3NxISEt7ahqzrjYqKyrNMeno6nj59qlK+JFhZWaFz58549uyZvKZPftb2SU9Px8aNGwG8Win6//7v/1CjRg0YGxvL3wNNe3wKS2ZmJvr164eEhATo6ekhPj4eQ4YMKelmEZUIhh/SWY0aNZL/sv/222/zDAp5qV69ujwf5MKFC7mWSUtLk4d/inOV4KxegrxCWX7n8zg6OsLX1xdr1qzBhQsXYGZmhtu3b791iAcAatWqBQC4d+9enr1ely9fltdBKulVlLPCzp07d2BsbIw+ffq8tU50dLQcBL28vHItc/To0UJroyZmzZqFgwcPws7ODgcPHoSJiQm2b9+ONWvWlHTTiIodww/pLEmSMGvWLADAkSNH5H+/yZ9//onw8HAAr1ZXbt26NQBg4cKFuZZfv349njx5AkNDQ7Rr165wGp4PWQ84PXLkSK77V65cWeBjVq5cWZ6T8+DBg7eWb9myJczNzZGamoply5blWmb+/PkAgDp16sgLGpaU9u3bo0uXLvD29sbXX3+dr56o7BOFs3r4srt8+TL+/PPPt9bPWrG7qPz777+YMWMG9PT0sGnTJrRq1QrfffcdAODLL7/E5cuXi/T8RO8ahh/SaR999JF8p9bUqVPRt29f+blZWTIzMxEWFobevXujU6dOKk/gnjp1KvT09HDgwAFMmDABKSkp8r6goCD52EOGDIG9vX3RX9B/OnfuDOBVWNu0aZO8PT09HQsXLswz/GzatAmTJ0/O8cswMzMTGzduxMWLFwEATZo0eWsbjI2NMW7cOADA7NmzsW3bNnlfRkYGFi5ciC1btgD43wM+S5Kenh52796N/fv3q6yw/CZKpVJeZfnLL7/E8+fP5X2nTp2Cj48P9PX186xfvXp1AOoNu+bXs2fP4Ovri4yMDEyYMAHe3t4AXk3m7tevH168eIHevXsXeQAjeqeU8K32RO+EJUuWyIsKAhB2dnbCzc1NNGjQQFhYWMjbnZ2dxa1bt1Tqfv/99yorPDdp0kQ4Ojrme4XnvNZ4yb7K8evets6PEEL06tVLPoe9vb1o0qSJMDc3F/r6+mLDhg25nj/7wo+WlpbC1dVVuLm5CWtra3n78OHDVc7zthWefX19VdrRtGnTAq3wnJes9Xr69++fZ5k31ctrbaa8ZLX39XV+goODhYGBgQAgjI2NRcOGDUX16tUFAFG5cmUxZ84cAUB4enrmOObChQvl49auXVu0atVKeHp6qqzPlJ81m4TIe50fHx8fAUA0b95cZbVwIV4tdOjs7CwAiCFDhhTk7SDSauz5IQIwatQoREREYPbs2fLcjQsXLuDGjRuwtraGr68vdu7cicuXL6NatWoqdYcNG4YTJ07A19cXpqamCA8PR2xsLFq1aoW1a9fir7/+QtmyZYv9mjZv3oyAgADUqlULT58+xa1bt9C8eXOEhITk+ayq7t27Y9GiRfDx8YG5uTlu3LiBc+fOwcjICF26dMEff/yB5cuX57sNenp62Lx5M3bs2IF27dohJSUFZ86cgYGBAT766CP8888/mDlzZmFdcolo06YNQkJC0L59exgYGODKlSsQQmDUqFE4c+aMfLdfbr766issXLgQDRo0wN27d/Hvv/8iJCQk33fUvc3ixYuxZ88emJubY+vWrTnWLCpXrhy2b98OhUKBVatWyWsSEZV2khAFnOVJREREpMXY80NEREQ6heGHiIiIdArDDxEREekUhh8iIiLSKQw/REREpFMYfoiIiEinGLy9CL3rMjMz8fDhQ5iamsoPUyQioqInhEBCQgLs7e2hp1c8/QkvX75848OJ88PIyAhlypQppBZpH4afUuDhw4dwcHAo6WYQEemsyMhIVKpUqcjP8/LlS5Q1tQLSX2h0HDs7O0REROhsAGL4KQVMTU0BAEZ1+kPSNyrh1hAV3L1DuT8YluhdlxAfj+pVHeSfw0UtNTUVSH8BRd2BgLo/7zNSEXVpHVJTUxl+SHtlDXVJ+kYMP6SV8vMEdaJ3WbFPOdDg5z0f68DwQ0REpH0kAOoGLk4NZfghIiLSOpLeq5e6dXUcww8REZG2kSQNen7Y9cP4R0RERDqFPT9ERETahsNeGmH4ISIi0jYc9tIIww8REZHW0aDnhzNe+A4QERGRbmHPDxERkbbhsJdGGH6IiIi0DSc8a4TvABEREekU9vwQERFpGw57aYThh4iISNtw2EsjDD9ERETahj0/GmH8IyIiIp3Cnh8iIiJtw2EvjTD8EBERaRtJ0iD8cNiL4YeIiEjb6EmvXurW1XHs+yIiIiKdwp4fIiIibcM5Pxph+CEiItI2vNVdI4x/REREpFPY80NERKRtOOylEYYfIiIibcNhL40w/BAREWkb9vxohO8AERER6RT2/BAREWkbDntphOGHiIhI23DYSyMMP0RERNqGPT8aYfwjIiIincKeHyIiIq2jwbAX+z0YfoiIiLQOh700wvhHREREOoU9P0RERNpGkjS424s9Pww/RERE2oa3umuE4YeIiEjbcM6PRhj/iIiISKew54eIiEjbcNhLIww/RERE2obDXhph/CMiItI2WT0/6r4K6N9//0WnTp1gb28PSZKwe/dulf1CCPj7+8Pe3h5ly5aFl5cXLl26pFImJSUFI0aMgLW1NUxMTNC5c2fcv39fpUxMTAz8/PygVCqhVCrh5+eH2NhYlTL37t1Dp06dYGJiAmtra4wcORKpqakFuh6GHyIiInqjpKQkNGjQAMuXL891//z587F48WIsX74cJ0+ehJ2dHdq2bYuEhAS5zKhRo7Br1y5s27YNoaGhSExMhI+PDzIyMuQyvr6+CA8Px759+7Bv3z6Eh4fDz89P3p+RkYEPP/wQSUlJCA0NxbZt27Bz506MGTOmQNfDYS8iIiJtU8zDXh07dkTHjh1z3SeEwNKlSzFp0iR069YNALB+/XrY2tpiy5YtGDp0KOLi4rB27Vps3LgRbdq0AQBs2rQJDg4O2L9/P9q3b48rV65g3759OH78ONzd3QEAq1evhoeHB65du4aaNWsiKCgIly9fRmRkJOzt7QEAixYtwoABAzB79myYmZnl63rY80NERKRlJEnS6AUA8fHxKq+UlBS12hIREYGoqCi0a9dO3qZQKODp6YmjR48CAE6fPo20tDSVMvb29qhXr55c5tixY1AqlXLwAYBmzZpBqVSqlKlXr54cfACgffv2SElJwenTp/PdZoYfIiIiHeTg4CDPrVEqlQgICFDrOFFRUQAAW1tble22trbyvqioKBgZGcHCwuKNZWxsbHIc38bGRqXM6+exsLCAkZGRXCY/OOxFRESkZbL34KhRGQAQGRmpMkykUCg0blN2Qoi3tvH1MrmVV6fM27Dnh4iISNtIGr4AmJmZqbzUDT92dnYAkKPn5cmTJ3IvjZ2dHVJTUxETE/PGMo8fP85x/OjoaJUyr58nJiYGaWlpOXqE3oThh4iISMsUxpyfwlK1alXY2dkhODhY3paamoqQkBA0b94cAODm5gZDQ0OVMo8ePcLFixflMh4eHoiLi8OJEyfkMmFhYYiLi1Mpc/HiRTx69EguExQUBIVCATc3t3y3mcNeRERE9EaJiYm4efOm/HVERATCw8NhaWmJypUrY9SoUZgzZw6cnZ3h7OyMOXPmwNjYGL6+vgAApVKJQYMGYcyYMbCysoKlpSXGjh0LFxcX+e6v2rVro0OHDhg8eDBWrlwJABgyZAh8fHxQs2ZNAEC7du1Qp04d+Pn5YcGCBXj+/DnGjh2LwYMH5/tOL4Dhh4iISOsUxpyfgjh16hRat24tfz169GgAQP/+/REYGIhx48YhOTkZw4YNQ0xMDNzd3REUFARTU1O5zpIlS2BgYIBevXohOTkZ3t7eCAwMhL6+vlxm8+bNGDlypHxXWOfOnVXWFtLX18eff/6JYcOGoUWLFihbtix8fX2xcOHCgr0FQghR4HeB3inx8fFQKpVQuAyGpG9U0s0hKrCYk7kvnEb0rouPj4etlRJxcXEF6nnQ5HxKpRLluv0IybCsWscQaclI/PX/iq3N7yL2/BAREWmZ4u75KW044ZmIiIh0Cnt+iIiItE22W9bVqqvjGH6IiIi0DIe9NMNhLyIiItIp7PkhIiLSMq8e6q5uz0/htkUbMfwQERFpGQmarNTM9MPwQ0REpGU450cznPNDREREOoU9P0RERNqGt7prhOGHiIhI22gw7CU47MXwQ0REpG00mfOj/kTp0oNzfoiIiEinsOeHiIhIy7DnRzMMP0RERNqGE541wmEvIiIi0ins+SEiItIyHPbSDMMPERGRlmH40QzDDxERkZZh+NEMww9pDdfaDvBuVguN61ZBExdH2NuY42VKGiyafZVr+Uq25viglQuauDiicd0qqOFoAz09PXh+shAnLtzJtc57bs4IWvNlnm04cT4Cnv0X5bqvoo05pnz+Idq1qAMLM2NERsVgx9+nMX/t30hJTX/r9VmYGSN81xTYWJriWkQUGnab9dY6RADw/PlzNKxXC9HR0ahRsybOXbyao8ysGf6YPXN6nscY8/V4zJoztwhbSfTuYPghrTFxcAd0at0g3+W7ejfEgq97qHWuW/eicTT8Vo7tt+8/zbV81UrWOLR+DGwsTXHxxkMcOXMTjepUxjdDOuJ995poP3gZUtPeHIDmjekGa3MTtdpLum382NF4+jT3z+brPJq3gJNT9RzbXRu5FXazqAix50czDD+kNcLOR+D89Qc4fekeTl+6i7sHAt5YPuLBMyzb9A9OX7qLU5fuYsXUfmjV2Dlf5zoafgtDpm3Kd9tW+veDjaUpvt9yEGMX7AQA6OvrYfO8T9HFuyHGDWqHWT/+lWd9r6Y14Ne5Gdb8EorPerTM93mJDv5zAJs2rsegz4Zg7ZpVby0/8NPP4Nd/QNE3jIoWb3XXCG91J62xKHA/Zv34F/YevognzxPeWv7PkAsYv+hX/LzvNG5H5u+vYnW41amM99yc8fhZPL5Z+pu8PSMjEyPnbEdqWjo+7+MJA4Pc/3crozDEd5P64PKtR1i68UCRtZNKn+TkZIwY/n+oXacORo0eW9LNoWKU1fOj7kvXMfwQaahjq3oAgL/+vZhjaOvJ8wQcOXMLlkoTeDRwyrX+pKEdUa2SNUbO2Ya0tIwiby+VHrNnTsftW7ewbPkKGBoalnRziLQGh72IclG9cnnMGNEZlkoTPItNxNHwWwg6cgVCiBxl69eoCAAIvxKZ67HCr0aitXtN1K9REYdP31DZV8/ZHl9+7I0Nvx/HkTO3ULmCZeFfDJVKF86fx7dLFuGT/gPR8r1WuHvnTr7qHTr4D86dC0fKy5eoWKkS2rXviEZunO+jbTjnRzMMP0S58GjoBI+Gqj01F64/QN+v1+DWvWiV7ZXsXgWWB09icz3Wg8evtjtUsFDZLkkSfpjii9jEF5iUbbiM6G0yMzMx7P8Gw9zcHLPnzi9Q3S2bN6p8PX3aFHTt1h2r1waiXLlyhdlMKkIMP5rhsBdRNvGJyVgcGIxWfgtg7zkO9p7j0GHIMoSdj4BLjYr4c8UXMCtXRqVOOWMFAODFy9Rcj5n0MgUAYFJWobJ9WB9PNHFxxDdLduN5XFIRXA2VVj8s/w6nTp7AnLkLYGVlla86Tk7VETB/Ic6cu4SnsYm4ERGJdRs2w75iRez+dSc+HeBXxK0menew54com3PX7uPctfsq20JOXsf7Axfj79VfomWj6hjaqxUW/BQk78/6Iyq3ITEAkHK5taKSrTmmDffBv6duYNMfYYV3AVTqRUZGYvq0yXivlWeB7trq2+9jla9NTEzQp68vPL1ao4mrC/74bTeOHT0Kj+bNC7nFVCR4t5dG2PPzDgkMDIQkSQgMDCzpptBrMjMFFq0LBgC0bV5bZV9CUu49O1mMyxgBAJKSU+RtSyf2hpGhPkbO2VYUzaVSbNSIYUhNTcWy5SsK5XgVKlSAX/+BAID9wX8XyjGp6PFuL82w54con27+N9fHzlqpsv1+1HO41nZARRvzXOtVtH21PfJRjLztQ08XxMS/wLJv+qiULWP06n9JBztL/L361UrT3UauQFJy7kNqpHv++nMPzM3NMfKLz1W2v3z5EgAQee8e2nl7AQB+/W1PvubxVK/+av2rqEePCrexVGQ450czDD9E+WRhVhYAkPgiRWX7+esP0Kl1AzSs7ZBrvYa1Xm2/cOPBa8czznPRReOyRvI+A319jdpNpU9sbCwO/xuS677k5GR5X3r62x+rAgAxsa+CuQknPJOOYPghyqeu3g0BAGev3FPZvu/wJUwa+gE+aFUPRoYGKmv92FiaokUjJ8QmvFB5XEZZ1y9yPUflCpa49tcMPtuL8pSclvvcsrt37qCWc9U8n+2VFyEEft+9CwAfcaFNJGjQ88NJP+/mnJ9Dhw5BkiT4+/vjzJkzaN++PUxNTaFUKvHRRx/hTi7rWRw9ehQffvghLC0tUaZMGdSqVQv+/v548eJFjrKSJMHLywsPHjzAgAEDYGdnBz09PRw6dEjl3EePHkXr1q1hamqK8uXLY9iwYUhOTgYA7Nu3Dy1atICJiQlsbW0xfvx4ZGSoLlAXFxeHefPmwdPTE/b29jAyMoK9vT0++eQT3LqV87lRVPIGdW8BS2XO52sN6t4CI/q9j8zMTKz5JVRl36lLd3H07C3YWplh9pdd5O36+nr49pveMDI0wIptIUhPzyzy9hPl5unTp9i8cQNSUlR7LRMTEzFy+Oc4eSIMdnZ26NL1oxJqIRUU5/xo5p3u+Tl16hQWLFgALy8vDB06FGfPnsXu3btx4cIFXLx4EWXKvLrleOfOnejTpw+MjIzQu3dv2NjYYP/+/Zg+fTqCgoJw8OBBKBSqk1GfPXsGDw8PWFpaonfv3khNTYWZmRni4+MBAGFhYZg3bx7at2+PoUOH4uDBg1ixYgXi4+PRpUsX9O/fH507d4a7uzv+/PNPzJ8/H2ZmZpg0aZJ8jitXrmDq1Klo3bo1PvroI5iYmODq1avYsmUL/vzzT5w5cwZVqlQpvjdUy3VoWRcTB3dQ2WZkqI+Q9WPkrwNW78O+0EsAADtrM2xfNFjeV6uaHQDgh6m+SPpv6Gpv6CXMXb1PLvP1p+2weHxPXLkdhXuPngMA6lW3R9VK1sjIyMTYBTtxNpfFDIf4b8KhwDH4ol9reDapgasRUXCrUxnVHMrjxPkIzFvDiaRUcpISE/HZp/0xetQI1KxVGw6VKyMuNhbhZ8/g2bNnMDc3x+Ztv8DY2Likm0r5xbu9NPJOh58///wT27ZtQ+/eveVtn3zyCTZu3Ijdu3ejT58+SEhIwGeffQZ9fX0cO3YM9evXB/CqK/fjjz/Gli1bsGDBAkyePFnl2BcvXsTAgQOxevVq6GebU3Ho0CEAr3p2du/ejS5dXv0ln5aWhsaNG2PLli34+++/ERISgiZNmgAApk+fjurVq2PJkiUYP348DAxeva21a9fGo0ePYGmpumrvwYMH0aZNG8yaNQurV68u8PuSkpKi8hdcVmAr7awtyqFp/aoq2/T09FS2WVv8b86CkaFBjvIAULe6vfzva3ceq+z7duM/8G5WC3WcKqB105owNNBD1NN4bNlzAj9sPYTTl++9fjgAr54C79F3LqZ87oO2zWujs2N93I+KRcDqvZi/Nggpqfmbe0FUFCytrDDm6/E4EXYct27dxPlz4dDX14dj1ar4+JMBGPHlV6hYsWJJN5Oo2LzT4adVq1YqwQcAPv30U2zcuBEnT55Enz59sHv3bsTGxuLzzz+Xgw/wqktw7ty5+PnnnxEYGJgj/BgZGWH+/PkqwSc7Ly8vOfgAgKGhIXr06IHz58+jU6dOcvABAFNTU/j4+OCnn37C/fv34ejoCABQKpWvHxYA0Lp1a9StWxf79+8v0PuRJSAgANOnT1errjbb9EdYgdbEuffoeZ5za/KyYlsIVmzLfSLp29x/HIuh/vl/Enxu1GkzEQBUcXTMcz6QqakpZs2ZW8wtoqLEu700807O+cnSqFGjHNsqVaoE4NXdDgBw9uxZAK/CyuscHBzg5OSEW7duISFB9SngVatWhbW1dZ7ndnV1zbGtQoUKAICGDRvmue/BA9U7eg4dOoSuXbuiQoUKMDQ0lD+wFy5cwMOHD/M8/5tMnDgRcXFx8isyMvdnShERUenEOT+aead7fnLrOckaUsqaXJw15GNra5vrMezs7HDt2jXEx8fD1NRU3p5X+SxmZmZ5nvtN+9LS0uRtO3bsQO/evVGuXDm0b98ejo6OMDY2lhcyvHv37hvbkBeFQpFjDhMRERHlzzsdfvIjK4g8fvw41/1Z218PLMWRfP39/VGmTBmcPn0azs6q67ls28aVfYmISD2S9L9H66hTV9e908Ne+ZE1PJU1UTm7Bw8e4NatW6hWrZpKr09xuXXrFmrXrp0j+Dx8+JC3uhMRkdpehR91h71KuvUlT+vDT5cuXaBUKrFu3TpcunRJ3i6EwMSJE5GWloYBAwaUSNuqVKmCmzdvqvRKvXz5Ep9//nm+V14lIiLKQfpf709BX7zVvZQMe61evRp9+/aFu7s7evfujfLly+PAgQM4deoUmjZtiq+//rpE2jZixAiMGDECrq6u6NGjB9LT0xEcHAwhBBo0aIBz586VSLuIiIh0mdb3/ABAz549cfDgQbRq1Qq//vorlixZgvj4eEyZMgX//POPvBhicRs+fDh+/PFHWFpaYvXq1di1axc8PT1x9OhRmJubl0ibiIhI+/FuL81IQojcF4YgrREfHw+lUgmFy2BI+kYl3RyiAos5ubykm0Cklvj4eNhaKREXF5frncBFcT6lUonqo3ZCX5HzUTz5kZGShJtLuxdbm99FWj/sRUREpGv09CTo6anXgyPUrFealIphLyIiIqL8Ys8PERGRluE6P5ph+CEiItIyfLaXZjjsRURERDqFPT9ERERahsNemmH4ISIi0jIc9tIMh72IiIi0THEucpieno7JkyejatWqKFu2LKpVq4YZM2YgMzNTLiOEgL+/P+zt7VG2bFl4eXmpPHIKAFJSUjBixAhYW1vDxMQEnTt3xv3791XKxMTEwM/PD0qlEkqlEn5+foiNjVX7fcoLww8RERHlad68efjxxx+xfPlyXLlyBfPnz8eCBQvw3XffyWXmz5+PxYsXY/ny5Th58iTs7OzQtm1bJCQkyGVGjRqFXbt2Ydu2bQgNDUViYiJ8fHyQkZEhl/H19UV4eDj27duHffv2ITw8HH5+foV+TRz2IiIi0jKFMecnPj5eZbtCoYBCochR/tixY+jSpQs+/PBDAICjoyO2bt2KU6dOAXjV67N06VJMmjQJ3bp1AwCsX78etra22LJlC4YOHYq4uDisXbsWGzduRJs2bQAAmzZtgoODA/bv34/27dvjypUr2LdvH44fPw53d3cAwOrVq+Hh4YFr166hZs2a6l1wLtjzQ0REpGUkaDDs9d9j3R0cHOThJaVSiYCAgFzP1bJlSxw4cADXr18HAJw7dw6hoaH44IMPAAARERGIiopCu3bt5DoKhUJ+liUAnD59GmlpaSpl7O3tUa9ePbnMsWPHoFQq5eADAM2aNYNSqZTLFBb2/BAREWmZwuj5iYyMVHm2V269PgAwfvx4xMXFoVatWtDX10dGRgZmz56Nvn37AgCioqIAALa2tir1bG1tcffuXbmMkZERLCwscpTJqh8VFQUbG5sc57exsZHLFBaGHyIiIh1kZmaWrwebbt++HZs2bcKWLVtQt25dhIeHY9SoUbC3t0f//v3lcq9PpBZCvHVy9etlciufn+MUFMMPERGRlinOW92//vprTJgwAX369AEAuLi44O7duwgICED//v1hZ2cH4FXPTYUKFeR6T548kXuD7OzskJqaipiYGJXenydPnqB58+ZymcePH+c4f3R0dI5eJU1xzg8REZGWyRr2UvdVEC9evICenmpc0NfXl291r1q1Kuzs7BAcHCzvT01NRUhIiBxs3NzcYGhoqFLm0aNHuHjxolzGw8MDcXFxOHHihFwmLCwMcXFxcpnCwp4fIiIiylOnTp0we/ZsVK5cGXXr1sXZs2exePFifPrppwBe9SSNGjUKc+bMgbOzM5ydnTFnzhwYGxvD19cXAKBUKjFo0CCMGTMGVlZWsLS0xNixY+Hi4iLf/VW7dm106NABgwcPxsqVKwEAQ4YMgY+PT6He6QUw/BAREWmd4hz2+u677zBlyhQMGzYMT548gb29PYYOHYqpU6fKZcaNG4fk5GQMGzYMMTExcHd3R1BQEExNTeUyS5YsgYGBAXr16oXk5GR4e3sjMDAQ+vr6cpnNmzdj5MiR8l1hnTt3xvLly9W6zjeRhBCi0I9KxSo+Ph5KpRIKl8GQ9I1KujlEBRZzsvB/uBEVh/j4eNhaKREXF5evycOFcT6lUolGU/ZAv4yJWsfIeJmEMzN9iq3N7yL2/BAREWkZPttLM5zwTERERDqFPT9ERETaRoNFDsGOH4YfIiIibcNhL80w/BAREWmZwni8hS7jnB8iIiLSKez5ISIi0jIc9tIMww8REZGW4bCXZjjsRURERDqFPT9ERERahsNemmH4ISIi0jIMP5ph+CEiItIynPOjGc75ISIiIp2Sr56fGTNmqH0CSZIwZcoUtesTERGRKg57aSZf4cff3x+SJEEIUeATMPwQEREVLg57aSZf4WfdunVF3Q4iIiLKJ/b8aCZf4ad///5F3Q4iIiKiYsG7vYiIiLSMBA2GvQq1JdpJ4/ATHh6OkydP4unTp6hbty46d+4MAEhJSUFKSgrMzMw0biQRERH9j54kQU/N9KNuvdJE7Vvdr1y5gmbNmsHNzQ3/93//h8mTJ2P37t3y/p9++gkWFhbYt29fYbSTiIiIqFCoFX7u3r2LVq1a4cSJE+jSpQvmz5+f406wPn36wNDQEDt37iyUhhIREdErWXd7qfvSdWqFn+nTp+P58+dYv349fv31V4wZMyZHGQsLC9SpUwfHjh3TuJFERET0P1l3e6n70nVqhZ+///4brq6u8PPze2O5KlWq4OHDh2o1jIiIiHKnJ2n20nVqhZ9nz57BycnpreUkScLLly/VOQURERFRkVDrbi9ra2tERES8tdyVK1dQsWJFdU5BREREeZE0WKyQPT/q9fx4enri9OnTOHLkSJ5l9uzZg2vXrqFt27ZqN46IiIhy4oRnzagVfiZOnAhDQ0N06tQJa9euRXR0tLwvMTERmzZtwsCBA2FsbJzrZGgiIiKikqJW+KlXrx42b96M1NRUDBkyBHZ2dpAkCRs2bIBSqUT//v3x4sULbNy4MV9zg4iIiCj/JA3/03VqL3LYvXt3XLx4ESNGjEDt2rVRtmxZGBoaolq1ahgyZAjOnTuHjz76qDDbSkRERODdXprS6PEWjo6OWLp0aSE1hYiIiPKDT3XXjNo9P0RERETaSKOen9TUVOzatQuhoaHyYob29vZo0aIFPvroIygUikJpJBEREf2PJndtseNHg/Bz4MABDBgwAA8fPszxXK8ffvgBFSpUwLp163irOxERUSHjU901o1b4CQsLw4cffojU1FS4u7ujb9++cHR0hBAC9+7dw9atW3H8+HF06tQJISEhcHd3L+x2ExER6Sz2/GhGrfAzZcoUpKWlYcWKFRg6dGiO/SNGjMCqVavwf//3f5g6dSr+/vtvjRtKREREVBjUmvAcFhaGxo0b5xp8sgwZMgRNmjTB8ePH1W4cERER5cSnumtGrfCjp6eH6tWrv7Vc9erV+SYTEREVMj7eQjNqhZ+mTZvi/Pnzby13/vx5NG3aVJ1TEBERERUJtcLPzJkzcePGDUydOhWZmZk59gshMG3aNNy4cQMzZ87UuJFERET0P1l3e6n70nX5mvC8YcOGHNv69++P2bNnY9OmTejevTuqVKkCALh79y527tyJu3fvYvDgwbh27Rrv9iIiIipE0n8vdevqOkm8vkhPLvT09HKdu5O9atb+1w8nSRIyMjI0bSe9QXx8PJRKJRQugyHpG5V0c4gKLObk8pJuApFa4uPjYWulRFxcHMzMzIrlfEqlEt1/PAzDsuXUOkZaciJ2/t97xdbmd1G+en6mTp3KictERERUKuQr/Pj7+xdxM4iIiCi/NHk6O5/qruGzvYiIiKj48anummH4ISIi0kLMMOrTKPyEhobit99+w40bN5CQkJBjsjPwKmEeOHBAk9MQERERFRq1wo8QAoMGDcL69evlwCNJUo67v4QQ7F4jIiIqZBz20oxaixz++OOPCAwMhJubG4KDg9GtWzcAwLVr17B3714MGDAAenp6+Prrr3H79u1CbTAREZGuy5rwrO5L16nV8xMYGAgTExPs3bsXVlZW2LRpEwDA2dkZzs7OaN++PT744AP07t0bzZs3lxdAJCIiIippavX8XLlyBR4eHrCysgLwvy607IsZ9ujRA25ubli4cGEhNJOIiIiy8KnumlEr/GRmZsLa2lr+2tjYGAAQExOjUs7Z2RkXLlzQoHlERET0OknDV0E9ePAAH3/8MaysrGBsbIyGDRvi9OnT8n4hBPz9/WFvb4+yZcvCy8sLly5dUjlGSkoKRowYAWtra5iYmKBz5864f/++SpmYmBj4+flBqVRCqVTCz88PsbGxarT4zdQKPxUrVlRpcNaw1tmzZ1XKXb9+HQYGvJueiIioMBXng01jYmLQokULGBoaYu/evbh8+TIWLVoEc3Nzucz8+fOxePFiLF++HCdPnoSdnR3atm2LhIQEucyoUaOwa9cubNu2DaGhoUhMTISPj4/KqJGvry/Cw8Oxb98+7Nu3D+Hh4fDz89P4/XqdWuGnUaNGuHz5MtLT0wEA7dq1gxACX3/9Na5cuYKEhAQsWLAAp0+fhqura6E2mIiIiIrPvHnz4ODggHXr1qFp06ZwdHSEt7c3nJycALzq9Vm6dCkmTZqEbt26oV69eli/fj1evHiBLVu2AADi4uKwdu1aLFq0CG3atIGrqys2bdqECxcuYP/+/QBeTanZt28f1qxZAw8PD3h4eGD16tXYs2cPrl27VqjXpFb46dy5M54/f449e/YAABo0aIA+ffrg/PnzqFevHszNzTFhwgQYGBhg9uzZhdpgIiIiXSdJmr2AVw9Jzf5KSUnJ9Vy///47GjdujJ49e8LGxgaurq5YvXq1vD8iIgJRUVFo166dvE2hUMDT0xNHjx4FAJw+fRppaWkqZezt7VGvXj25zLFjx6BUKuHu7i6XadasGZRKpVymsKgVfvr27Yvk5GR06tRJ3rZ+/XrMmTMHTZo0QfXq1fHBBx/gwIEDaNq0aaE1loiIiApnwrODg4M8t0apVCIgICDXc92+fRsrVqyAs7Mz/v77b/zf//0fRo4ciQ0bNgAAoqKiAAC2trYq9WxtbeV9UVFRMDIygoWFxRvL2NjY5Di/jY2NXKawqD0hR6FQqHxtaGiICRMmYMKECRo3ioiIiPKWvQdHnboAEBkZCTMzM3n767/Xs2RmZqJx48aYM2cOAMDV1RWXLl3CihUr8Mknn2Q7rmqD8rPQ8etlcitfFAsmq9XzQ0RERNrNzMxM5ZVX+KlQoQLq1Kmjsq127dq4d+8eAMDOzg4AcvTOPHnyRO4NsrOzQ2pqao67wl8v8/jx4xznj46OztGrpCmGHyIiIi1TnHd7tWjRIseE4+vXr8t3eletWhV2dnYIDg6W96empiIkJATNmzcHALi5ucHQ0FClzKNHj3Dx4kW5jIeHB+Li4nDixAm5TFhYGOLi4uQyhSVfw17VqlVT+wSSJOHWrVtq1yciIiJVhTHslV9fffUVmjdvjjlz5qBXr144ceIEVq1ahVWrVv13PAmjRo3CnDlz5Cc9zJkzB8bGxvD19QUAKJVKDBo0CGPGjIGVlRUsLS0xduxYuLi4oE2bNgBe9SZ16NABgwcPxsqVKwEAQ4YMgY+PD2rWrKnexeYhX+Hnzp07hXpSIiIi0g5NmjTBrl27MHHiRMyYMQNVq1bF0qVL0a9fP7nMuHHjkJycjGHDhiEmJgbu7u4ICgqCqampXGbJkiUwMDBAr169kJycDG9vbwQGBkJfX18us3nzZowcOVK+K6xz585Yvnx5oV+TJLI/ip20Unx8PJRKJR5Fx6pMXiPSFslpGW8vRPQOSoiPR1V7K8TFxRXLz9+sn/efbToBI+Nyah0j9UUi1nzctNja/C7i8stERERaRg/qT9rlZF+GHyIiIq2jyQNK+WBTBkAiIiLSMez5ISIi0jKSBOgV091epRHDDxERkZbR0yD8qFuvNGH4ISIi0jKc86MZzvkhIiIincKeHyIiIi3DYS/NFEr4uXHjBp4+fQorKyvUqFGjMA5JREREeSjOx1uURmoPeyUnJ2P8+PGwsrJCrVq10LJlS8ydO1fev27dOjRq1Ajh4eGF0U4iIiKiQqFW+ElKSoKnpycWLlwIhUKBDz/8EK8/JaNVq1YIDw/H9u3bC6WhRERE9EpxPtW9NFIr/MybNw+nTp3C4MGDERERgd9//z1HGScnJ9SqVQv79+/XuJFERET0P3oavnSdWu/B9u3b4ejoiO+//x4KhSLPclWqVMH9+/fVbhwRERHllDXnR92XrlMr/Ny7dw9ubm4qj6HPjZmZGWJiYtRqGBEREVFRUOtuLxMTEzx9+vSt5SIiImBlZaXOKYiIiCgPelB/7o4e2PWjVs+Pm5sbTpw4gcjIyDzLXLp0CWfPnoWHh4fajSMiIqKcOOylGbXCzxdffIHk5GR069YNN2/ezLH/7t27+OSTT5CZmYkvvvhC40YSERHR/2QtcqjuS9epFX46deqEr776CqdPn0bNmjVRr149SJKEoKAgNG7cGM7Ozjh79izGjRsHLy+vQm4yERERkfrUvuNt0aJF2LZtG1xcXHD58mUIIfDw4UOcOXMGTk5O2LhxIwICAgqzrURERIRXQ1fqrvHDYS8NH2/Rq1cv9OrVC9HR0bh79y4yMjJQqVIlVKxYsbDaR0RERK/h4y00UyjP9ipfvjzKly9fGIciIiIiKlJ8qjsREZGW4VPdNaNW+Hn//ffzXVaSJBw4cECd0xAREVEupP/+U7eurlMr/Bw6dOitZSRJghACEgcXiYiIChV7fjSjVviJiIjIdXtmZiYiIyPx999/49tvv8Xw4cMxbNgwjRpIREREVJjUCj9VqlTJc1/VqlXRqlUrtG7dGh07dkSzZs3eWJ6IiIgKhj0/mimyJ9u3adMGbm5umDt3blGdgoiISCdJkqTRS9cVWfgBAAcHB1y6dKkoT0FERKRz+HgLzRRZ+ElOTsbJkydRpkyZojoFERERUYGpNefn3r17ee5LTEzE9evXsWjRIkRGRqJv375qN46IiIhy4grPmlEr/Dg6Or51zFAIgZo1a2LBggVqNYyIiIhyl/WcLnXr6jq1wk+rVq3yDD9GRkaoUKECPD090bdvXw57ERER0TulyBY5JCIioqLBW901o9aE52XLlmHNmjWF3RYiIiLKD+l/834K+uLTLdQMP2PGjMEff/xR2G0hIiKifNCDpNFL16kVfuzs7DiXh4iIiLSSWuGnffv2CA0NRWpqamG3h4iIiN5C3SEvTW6RL03UCj+zZ8+Gvr4++vXrh0ePHhV2m4iIiOgNuMKzZtS622vixIlo0KABfv31V/z5559o1KgRKleunOtQmCRJWLt2rcYNJSIiole4zo9m8hV+qlWrhp49e2LevHkAgMDAQHnfy5cvcfToURw9ejTXugw/RERE9C7JV/i5c+cOoqOj5a8PHjxYZA0iIiKiN+PjLTSj1rCXp6dnYbeDiIiI8kkPGgx78Vb3onuqOxEREdG7SK2eHyIiIio5HPbSTL7DT3h4OGbMmKHWSaZOnapWPSIiIspJD+oP3XDIpwDh59y5czh37lyBDi6EgCRJDD9ERESFSJIkSGp24ahbrzTJd/hxcnJCixYtirItREREREUu3+GnZcuW+Omnn4qyLURERJQPmjycnf0+nPBMRESkdbjCs2YYfoiIiLQQI4z6OOmbiIiIdArDDxERkZbJWudH3ZcmAgICIEkSRo0aJW8TQsDf3x/29vYoW7YsvLy8cOnSJZV6KSkpGDFiBKytrWFiYoLOnTvj/v37KmViYmLg5+cHpVIJpVIJPz8/xMbGatbgXOQr/GRmZnKyMxER0Tsi61Z3dV/qOnnyJFatWoX69eurbJ8/fz4WL16M5cuX4+TJk7Czs0Pbtm2RkJAglxk1ahR27dqFbdu2ITQ0FImJifDx8UFGRoZcxtfXF+Hh4di3bx/27duH8PBw+Pn5qd3evLDnh4iISAfFx8ervFJSUt5YPjExEf369cPq1athYWEhbxdCYOnSpZg0aRK6deuGevXqYf369Xjx4gW2bNkCAIiLi8PatWuxaNEitGnTBq6urti0aRMuXLiA/fv3AwCuXLmCffv2Yc2aNfDw8ICHhwdWr16NPXv24Nq1a4V67Qw/REREWkZPwxcAODg4yMNLSqUSAQEBbzzn8OHD8eGHH6JNmzYq2yMiIhAVFYV27drJ2xQKBTw9PXH06FEAwOnTp5GWlqZSxt7eHvXq1ZPLHDt2DEqlEu7u7nKZZs2aQalUymUKC+/2IiIi0jKFscJzZGQkzMzM5O0KhSLPOtu2bcOZM2dw8uTJHPuioqIAALa2tirbbW1tcffuXbmMkZGRSo9RVpms+lFRUbCxsclxfBsbG7lMYWH4ISIi0jKFscihmZmZSvjJS2RkJL788ksEBQWhTJkyeR/3tTCW9YirN3m9TG7l83OcguKwFxEREeXp9OnTePLkCdzc3GBgYAADAwOEhIRg2bJlMDAwkHt8Xu+defLkibzPzs4OqampiImJeWOZx48f5zh/dHR0jl4lTTH8EBERaZnivNvL29sbFy5cQHh4uPxq3Lgx+vXrh/DwcFSrVg12dnYIDg6W66SmpiIkJATNmzcHALi5ucHQ0FClzKNHj3Dx4kW5jIeHB+Li4nDixAm5TFhYGOLi4uQyhYXDXkRERFom+8RldeoWhKmpKerVq6eyzcTEBFZWVvL2UaNGYc6cOXB2doazszPmzJkDY2Nj+Pr6AgCUSiUGDRqEMWPGwMrKCpaWlhg7dixcXFzkCdS1a9dGhw4dMHjwYKxcuRIAMGTIEPj4+KBmzZpqXm3uGH6IiIi0TGFMeC5M48aNQ3JyMoYNG4aYmBi4u7sjKCgIpqamcpklS5bAwMAAvXr1QnJyMry9vREYGAh9fX25zObNmzFy5Ej5rrDOnTtj+fLlhd5eSQghCv2oVKzi4+OhVCrxKDo2X5PXiN41yWkZby9E9A5KiI9HVXsrxMXFFcvP36yf95uOXIdxOdO3V8jFi8QEfNyiRrG1+V3Enh8iIiItUxh3e+kyhh8iIiIto8kzuopg1Evr8G4vIiIi0ins+SEiItIyepCgp+YAlrr1ShOGHyIiIi3DYS/NMPwQERFpGem//9Stq+s454eIiIh0Cnt+iIiItAyHvTTD8ENERKRlJA0mPHPYi+GHiIhI67DnRzOc80NEREQ6heGHSrWzZ05j4YK56NurO6pXrQQThR4szcoW6BgfdmgLE4UeTBR6iIqKyledgNkz5Do7ft6mTtOJZI8fR2HS+DFo2rAOKlmborqDDd5v2RT+kyeolJs3ewasyxnm+Zox9Zscx7508TzGjx6J9q1boG71yrC3NEFVeyt0eL8l1vz4PdLT04vrMqkAsnp+1H3pOg57Uak2d84s7PnjN7Xrb9wQiEMHD0CSJOT3GcDXr13DgnkBBapDlJeTYcfQp3tnxMXGomatOmj/QSckJSbg2tUrWPHdUvjPmpujjrtHc1St5pRje4OGjXJsOxZ6GGtXrYBD5SqoWas2rKzL49nTaJw4fhSnToThzz9+w8+7/4ShoWGRXB+ph7e6a4bhh0q1ps2awaV+fbi5NUGjxk1QrXKFfNeNjo7GpAlfw7tNO9y4cQ337t59ax0hBEYMHwqluTmaNm2mUfAievToIfp074zUlBSs37IDH3buqrL/zKkTudb7uP+n6Ptx/3ydo037jjjVviMcq1ZT2f7k8WN079Qeh0MOYtP6nzDws6FqXQPRu4jDXlSqjRk7HpOnTkfHD31ga2tboLrjxn6FpKQkLF32fb7rBP60BqGH/0XAvIVQmpsXsLVEqmZO/QZxsbGYNjMgR/ABgEaNm2p8Dseq1XIEHwCwsbXFyDHjAAChIYc0Pg8VLj1Js5euY/ghykVw0N/4edsWjJvwDao55Rw+yE1UVBQmfzMeXq290advvyJuIZV2sTEx+O3XX2CmVOLjAYNKpA36evoAAEMjDnm9ayQN/9N1HPYies2LFy/w5Refo2bNWvjqv7988+Pr0V8iOTkZ3373QxG2jnRF2PGjSElJQavW3jA0NMTvu3Yi7NgRpKWlwblGTXTp1hM2efRmHg45iIvnz+Hly5ewr1gJ3u3ao6GrW4HOHxsTgx++WwIA8G7XQeProcLFW901w/BD9JoZ/lNw9+4d7A36B0ZGRvmqs/fPPfh15w5MnuqP6s7ORdxC0gXXrlwGANjY2MCnnRdOhh1X2T/LfzKWrViDLt165Kj789bNKl8HzJyGTl264buVa1GuXLlcz3fr5g0sWRCAzMxMRD95ghNhx5CUmIj+nw5Gj159C+mqiN4NDD9E2Zw9ewY/LF+Gfn790crTK191EhMTMerL4XB2roHRY8cXbQNJZ8TGxgAAtm/ZBIVCgW9/WIUOH3RCUlIiVv/4PVZ8txSff9Yf1WvUQN169QEAVZ2cMH3OfLRp2x6VKldBXGwMjh45jOmTJ+KP335FRkYGNmz7JdfzRT95jG2bN6ps+2zoMEyaNhMSuwreORLUv2uL300tnfOTmpqKyZMnw8nJCUZGRpAkCYcOHSrQMQIDAyFJEgIDA1W2Ozo6wtHRsdDaStojIyMDX3w+BObm5pgzd0G+6/lP+Qb3IyOx9LsfoFAoirCFpEsyMjIAAOnp6ZgRsAD9PhkIK2trVK7iiJkBC9C5a3ekpqbiuyUL5Tq9+vTD8JFfoWbtOjAxMYF9xUro0asvgv89BktLK/y15zecOH401/M1a94STxPT8DjuJU5fvI4ZAfOxfesmeLdqhnt37xTHJVMBcMKzZrQy/CxcuBCzZ89G5cqVMW7cOEybNo2BhTS2fNlShJ89g5lz5sHa2jpfdU6dPIGVP/6Avv384NX6/SJuIemScuVMAQB6enro0++THPt9PxkAADhy+N+3HsvOrgL6+r269f2f/cFvLKuvr48qjlUxbMRXWP7jWty+eQMTxo4qWOOpyHHCs2a0ctjrr7/+Qrly5RAUFKT2wlsfffQRmjVrhgoV8r/uC5Vue//aA0mSsGXTBmx9rfv/8X8rO/v27g4jIyNM9Z+J5i1a4u99fyEzMxOXLl5Ah7atVepcu3YVADB39kysXb0SXT/qjv8b9kXxXAxpvcpVqgAAbGztcu1RrFz51f6n0U/ydbxqTtUBAI+jHuW7DR906gKTcuVwIGgfUlNT8z0Hjuhdp5Xh5+HDh7CystJoxVGlUgmlUlmIraLSQAiB0Df8JR12/BgA4NnTpyrbz58Lz7PO1atXcPXqFbjUb1AobSTd4FK/IQAgLjYGQogc826eP38GADDJYwLz6+JiYwtUHgAkSYKFhSXuJ95DbExMnneXUfHj3V6a0aphL39/f0iShIiICNy9exeSJEGSJHh5eSEuLg7z5s2Dp6cn7O3tYWRkBHt7e3zyySe4detWjmPlNecnNwMGDIAkSbhz506ebco+5+jQoUOQJAn+/v44duwY2rdvD3Nzc5UfXkII/PTTT2jRogXMzMxgbGyMxo0b46efflLnraFCsC/4IJJSMnN9Zf0VfuvuQySlZKJTl64AgElT/POs0++/YYbAjVuQlJKJBYuWltCVkTaqU88FVRyrIjk5GadPhuXYnzXcVb+B61uPJYTAn3/sBgA0cM35iIu83Im4jQf3I2FqZgarfA4FU/GQNHzpOq0KP15eXpg2bZrcazNt2jRMmzYNAwYMwJUrVzB16lSULVsWH330EUaNGoXGjRtjy5YtaNq0Ke7m49EEhe3o0aPw9PQEAAwZMgS9e/cG8OoH0ccff4xBgwbh6dOn8PX1xWeffYakpCQMGjQIY8eOLfa2EtG7Z8RXr34WTPz6K5XexvCzp+U1eAYMGgLgVW/k9i0bkZKSonKMxMREjP1yOE6fPAEbWzt82Kmryv5lSxbiTsTtHOe+cf0ahn7qByEEevf9GPr6+oV5aUQlSquGvby8vODl5SX31vj7+8v74uLi8OjRI1haWqrUOXjwINq0aYNZs2Zh9erVxdhaIDg4GGvXrsWnn36qsn3NmjXYsmULBg0ahB9//BEGBq++DampqejRowcWLVqEvn37ws0t90XJUlJSVH7AxcfHF91FaLl9f/2JuQGzVLalpqbC6z0P+esJEyejwwcfFnfTiN7qk4Gf4fChg/ht1y9o1qgumrh7ICkxESfDjiE1NRV+Awah80fdAQBJSYkYPuRTTBg7CjVq1kKlSpURFxeL8+Fn8fz5MyjNzbFu0zYYGxurnGPd6h8xa9ok1HWpj2rVqkMIgcjIuzh39gwyMzPh0eI9TJ4+uyQun95ADxL01By/0mPfj3aFnzfJa/5O69atUbduXezfv7+YWwS4urrmCD4AsHz5cpiYmGD58uVy8AEAIyMjzJ49G3/88Qe2bt2aZ/gJCAjA9OnTi6zdpUn002icPKE6ZCCEUNkW/TS6uJtFlC96enpYvX4zWrzXChvX/4TQkIOQJAkNG7lhwKAh6NX3Y7mshaUVRo7+GqdOhCHi9i1cPH8O+vr6qFylKvp8/Ak+/+JLVLCvmOMc30ybgf1/70X42TP450AQXiYnw8LCEl7vt0G3nr3Rq+/H0NPTqkECnaDJ8BWjTykKP8CruTZLly5FWFgYnj59ivT0dHlfSdyl0LRpzocOvnjxAhcuXIC9vT3mzp2bY39aWhoA4OrVq3ked+LEiRg9erT8dXx8PBwcHAqhxaWP3ycD4PffLcGauHI9osB1Vq1Zh1Vr1ml8btJtenp6+HTI5/h0yOdvLGdqaoqpM+YU+Pg9e/uiZ29fdZtHJYXpRyOlJvzs2LEDvXv3Rrly5dC+fXs4OjrC2NhYntRcEnN+cnuKeEzMqzs3Hjx48Mbem6SkpDz3KRQKLqZHRESkplITfvz9/VGmTBmcPn0azq89W2nbtm0aHTuryzd7T1KWuLi4POvltiS8mZkZAMDNzQ2nTp3SqF1ERKSbNFmskIscatndXm9y69Yt1K5dO0fwefjwYa63uheEhYUFAODBgwc59p09e7ZAxzI1NUXt2rVx5coVxP637gYREVGBSP9b66egL2afUhR+qlSpgps3b+Lx48fytpcvX+Lzzz/PtcemIBo3bgwAOdYE+uWXXxASElLg440cORIvXrzA4MGDcx3eioiIyHVNISIiIoDr/Giq1Ax7jRgxAiNGjICrqyt69OiB9PR0BAcHQwiBBg0a4Ny5c2ofu2vXrqhatSoCAwMRGRkJV1dXXLlyBf/88w8++OAD/PXXXwU63tChQ3H8+HGsX78eR44cQZs2bWBvb4/Hjx/j6tWrCAsLw5YtW/i8MiIioiJQanp+hg8fjh9//BGWlpZYvXo1du3aBU9PTxw9ehTm5uYaHbts2bI4cOAAunTpghMnTmDFihV4+fIl/v33XzRp0qTAx8uahL19+3bUrVsXe/bsweLFixEcHIwyZcpg4cKFaNOmjUZtJiKiUoxdPxqRhBCipBtBmomPj4dSqcSj6Fh5QjWRNklOyyjpJhCpJSE+HlXtrRAXF1csP3+zft4fPBeJcqbqnS8xIR6tGzgUW5vfRaWm54eIiIgoP0rNnB8iIiJdwae6a4bhh4iISMtwgWfNMPwQERFpG6YfjXDODxEREekU9vwQERFpGT7eQjMMP0RERFqGE541w/BDRESkZTjlRzOc80NEREQ6hT0/RERE2oZdPxph+CEiItIynPCsGQ57ERERkU5hzw8REZGW4d1emmH4ISIi0jKc8qMZhh8iIiJtw/SjEc75ISIiojwFBASgSZMmMDU1hY2NDbp27Ypr166plBFCwN/fH/b29ihbtiy8vLxw6dIllTIpKSkYMWIErK2tYWJigs6dO+P+/fsqZWJiYuDn5welUgmlUgk/Pz/ExsYW+jUx/BAREWkZScP/CiIkJATDhw/H8ePHERwcjPT0dLRr1w5JSUlymfnz52Px4sVYvnw5Tp48CTs7O7Rt2xYJCQlymVGjRmHXrl3Ytm0bQkNDkZiYCB8fH2RkZMhlfH19ER4ejn379mHfvn0IDw+Hn5+f5m/YayQhhCj0o1Kxio+Ph1KpxKPoWJiZmZV0c4gKLDkt4+2FiN5BCfHxqGpvhbi4uGL5+Zv18z7s6kOUM1XvfIkJ8XCvZa92m6Ojo2FjY4OQkBC0atUKQgjY29tj1KhRGD9+PIBXvTy2traYN28ehg4diri4OJQvXx4bN25E7969AQAPHz6Eg4MD/vrrL7Rv3x5XrlxBnTp1cPz4cbi7uwMAjh8/Dg8PD1y9ehU1a9ZU63pzw54fIiIiLSNp+AJeBansr5SUlHydOy4uDgBgaWkJAIiIiEBUVBTatWsnl1EoFPD09MTRo0cBAKdPn0ZaWppKGXt7e9SrV08uc+zYMSiVSjn4AECzZs2gVCrlMoWF4YeIiEgHOTg4yHNrlEolAgIC3lpHCIHRo0ejZcuWqFevHgAgKioKAGBra6tS1tbWVt4XFRUFIyMjWFhYvLGMjY1NjnPa2NjIZQoL7/YiIiLSNoVwt1dkZKTKsJdCoXhr1S+++ALnz59HaGhozsO+toCQECLHtte9Xia38vk5TkGx54eIiEjLFMaEZzMzM5XX28LPiBEj8Pvvv+PgwYOoVKmSvN3Ozg4AcvTOPHnyRO4NsrOzQ2pqKmJiYt5Y5vHjxznOGx0dnaNXSVMMP0RERJQnIQS++OIL/Prrr/jnn39QtWpVlf1Vq1aFnZ0dgoOD5W2pqakICQlB8+bNAQBubm4wNDRUKfPo0SNcvHhRLuPh4YG4uDicOHFCLhMWFoa4uDi5TGHhsBcREZGWKc7HWwwfPhxbtmzBb7/9BlNTU7mHR6lUomzZspAkCaNGjcKcOXPg7OwMZ2dnzJkzB8bGxvD19ZXLDho0CGPGjIGVlRUsLS0xduxYuLi4oE2bNgCA2rVro0OHDhg8eDBWrlwJABgyZAh8fHwK9U4vgOGHiIhI6xTnAs8rVqwAAHh5ealsX7duHQYMGAAAGDduHJKTkzFs2DDExMTA3d0dQUFBMDU1lcsvWbIEBgYG6NWrF5KTk+Ht7Y3AwEDo6+vLZTZv3oyRI0fKd4V17twZy5cvL/A1vg3X+SkFuM4PaTuu80PaqqTW+Tl945FG6/y4OVcotja/izjnh4iIiHQKh72IiIi0jDqPqcheV9cx/BAREWkbDSY8M/sw/BAREWmd4pzwXBpxzg8RERHpFPb8EBERaRt2/WiE4YeIiEjLcMKzZjjsRURERDqFPT9ERERapjgfb1EaMfwQERFpGU750QzDDxERkbZh+tEI5/wQERGRTmHPDxERkZbh3V6aYfghIiLSMhI0mPBcqC3RTgw/REREWoZTfjTDOT9ERESkU9jzQ0REpGW4zo9mGH6IiIi0Dge+NMFhLyIiItIp7PkhIiLSMhz20gzDDxERkZbhoJdmGH6IiIi0DHt+NMM5P0RERKRT2PNDRESkZfh4C80w/BAREWkbTvrRCIe9iIiISKew54eIiEjLsONHMww/REREWoZ3e2mG4YeIiEjLcMKzZjjnh4iIiHQKe36IiIi0DSf9aIThh4iISMsw+2iG4YeIiEjLcMKzZjjnh4iIiHQKe36IiIi0jvp3e3Hgi+GHiIhI63DYSzMc9iIiIiKdwvBDREREOoXDXkRERFqGw16aYfghIiLSMny8hWY47EVEREQ6hT0/REREWobDXpph+CEiItIyfLyFZhh+iIiItA3Tj0Y454eIiIh0Cnt+iIiItAzv9tIMww8REZGW4YRnzXDYi4iIiHQKe36IiIi0DOc7a4bhh4iISNsw/WiE4YeIiEjLcMKzZjjnh4iIiHQKe35KASEEACAhIb6EW0KknpdpGSXdBCK1ZP3czfo5XJznVfeuLf6uYPgpFRISEgAANapVLuGWEBHppoSEBCiVyiI/j5GREezs7OBc1UGj49jZ2cHIyKiQWqV9JFHccZUKXWZmJh4+fAhTU1NIXMCh0MXHx8PBwQGRkZEwMzMr6eYQFQg/v0VLCIGEhATY29tDT694ZpK8fPkSqampGh3DyMgIZcqUKaQWaR/2/JQCenp6qFSpUkk3o9QzMzPjLw/SWvz8Fp3i6PHJrkyZMjodXAoDJzwTERGRTmH4ISIiIp3C8EP0FgqFAtOmTYNCoSjpphAVGD+/RDlxwjMRERHpFPb8EBERkU5h+CEiIiKdwvBDREREOoXhh4iIiHQKww8RERHpFIYfoiKWkcGHdhIRvUsYfoiK0JkzZ7Bw4UJERUWVdFOIiOg/fLYXURF58eIF/P39sWfPHmRmZuLTTz+Fra1tSTeLiEjnMfwQFRFjY2NMnDgRGRkZmD59OjIyMjB48GAGINIaQghIkpTj30TajuGHqAh5eHjA398f6enpmDVrFgAwANE7LTMzE3p6r2ZECCGQnp4OSZJgYMBfF1R68PEWREUk+1/KJ0+exOTJkxESEoLJkyczANE7KXvw2bZtG4KCgnDlyhWYmJjA19cXLVq0QM2aNUu4lUSaY/ghKiYMQPQuyx7Wp0+fjpkzZ8La2hqVKlXC8+fPcefOHXh7e2P8+PFo06ZNCbeWSDO824uoiGVmZgIAmjRpglmzZsHT0xOzZs3C6tWr8fjx4xJuHdErWcHnp59+wsyZMzFgwAAEBQXh1KlTCA8PR8+ePXHgwAEsXrwYKSkpJdxaIs0w/BAVoqygk13WMALwvwDk5eWFWbNmYdWqVQxA9E4QQuD58+fYsmULatSogREjRqB+/foAgEOHDuHy5cuwsbHBypUroVAocv2sE2kLzmAjKiTZ50scOXIEN2/exIMHD+Dm5oaWLVvCxMQEwKsANHPmTEyZMgWzZ88GAAwZMoRDYFTssg91SZKEhIQEnD59Gn5+fmjQoAEAYPfu3Rg/fjxiY2MRFhYGBwcHAEBkZCTMzMxgYWFRYu0nUhd7fogKQfbgM3fuXHTp0gUDBw7E5MmT0blzZ3Tr1g0xMTFy+awA5OXlhdmzZ2Pt2rV49OhRSTWfdFBmZmaOW9efPXuGlJQUWFpaAgB++eUXTJgwQQ4+jo6OAIDk5GQMHDgQ+/fvL+5mExUKhh8iDQkh5OAzb948fPPNN3jvvffw888/Izo6Gj179kRwcDDee+89PH/+XK6XFYDatGmDyZMnY/PmzXwUBhWbrM/s2LFj4eHhAQCoVq0aqlWrhk2bNmHz5s2YPHkyYmJiVIIPACxZsgRHjx6FQqEoiaYTaYzhh0hDWX8979q1C8uWLYOfnx9mz56NHj16wMrKCtevX4epqSkuX76Mli1b5ugBmjRpErp16wYfHx/o6+uX1GWQDlq/fj1WrVoFhUKB69evw9zcHO+//z5u376NL7/8Es+fP8fVq1dVgs+vv/6KwMBAtGrVCs2bNy+5xhNpgLe6ExWCxMREDBo0CBcvXsT69evRuHFjZGZmomnTprh9+zZmz56N0NBQbN26FfXq1UNISIjKXImXL1+iTJkyJXgFpAuyD8+mp6dj/PjxuHTpEr777js4OzvL21u3bo0jR46gbdu2+Pvvv+X6q1atwrJly/D8+XMcOnQINWrUKJHrINIUJzwTFYIyZcrA3t4eTZo0kYNPhw4dcOPGDQQEBODzzz/H559/jpMnT+LixYt47733cOjQIVhbW8v1iYpaVvBZvHgxnjx5gh07dmDIkCFy8ElLS4OhoSF+/vln9O7dG8HBwbC0tISLiwuio6Nx+/ZtVKxYEUFBQQw+pNUYfogKgYGBAfz9/aFUKgEAP/zwAw4fPoyxY8diwIABcjlnZ2dIkoTLly/jgw8+wPHjx1VuhScqalFRUfjuu+8QFRUFpVIp372Vnp4OQ0NDAECFChVw8OBBzJw5EydOnMC1a9fg6OiIPn36oH///qhSpUpJXgKRxjjsRVQA2YcNsnv9oY/9+vXD3r17cePGDVhZWcnbW7Vqhfr168PCwgK9e/dGvXr1iqXdpLte/8xmZmbi9OnTGDduHEJCQvDee+9hx44dsLGxkT/HGRkZKvPPnj59Cmtr6zw//0Tahp9ionzK/oM/ODgYW7duxa5duxAXF6cSfFJSUhAXFwchBFJTU+XtW7duxdWrV9GxY0fMnDmTwYeKRdZn9smTJ/KdiY0aNcKCBQvQsmVLHD58GCtWrEBMTAwkSYIQAvr6+sj+d3HWre98qjuVFgw/RPmU9Utk1qxZaN++Pfr164fu3bvD3d0dYWFhcjmFQgEXFxfExcVh8ODBCA8Px9KlSzF37lwolUq4uLiU1CWQjlqwYAG8vLxw/vx5Odw0atQIS5YsgZubGxYvXow1a9bIAQhQDTpZn32GHyotOOxFVADbtm3DZ599htatW6N37944fPgwdu7cCUmS8P3336NLly7y2icffvgh9u7dK9etUqUK/vjjD/b4ULFKTk7GrFmzsHDhQri7u+O7775D/fr1IUkSMjMzcfbsWQwdOhQ3btzA5MmT8dlnn8HCwiLHUC5RacLwQ5QPWb8Ixo0bhzNnzuCHH35AjRo1kJGRge3bt2PmzJl48uQJVqxYAR8fHxgbGwN4tY7K48ePYW5ujg8++ACVKlUq4SshXfT06VOsXr0as2fPRv369bFixYo8A9DUqVMxcOBAeaiLqDRi+CHKQ26TOwcOHAgnJydMnjxZvi04MzMTv/76K6ZMmZJrACIqLm+akPz06VOsXLkSc+bMQYMGDXINQMOHD8eJEyfw7bffYvjw4ZzcTKUWb3UnykX2XyK7d+/G9evXkZCQgGvXrsnDVoaGhvJdMd27dwcATJkyBZ9//jn09PTQqVMneQiMQwhUVLI+g9kfs5KUlCQ/SDeLtbU1hg4dCkmSMHv2bAwfPhzff/89XFxcoKenB1dXV3z77beYMGEC2rVrx+BDpZsgojzNnDlTSJKk8mrTpo24du2aXCY9PV0IIURmZqbYsWOHcHFxEZIkiV9//bWkmk06IDIyUv531mdQCCFmz54t6tSpIx48eJBrvejoaDFx4kT5s3z27FmRkZEhhHj1GX758mXRNpzoHcBoT5SNyDYKvGfPHsyfPx89e/bEvn37sHDhQnh5eeHQoUNYu3YtHj58CADQ19dHRkYGJElC9+7dMX78eDRt2hR16tQpqcugUu7ixYuoUqUKhg8fDuDVZzA9PR3Jycm4e/curly5gr59++LRo0c56lpbW2PEiBFwcXHBgQMHMHbsWJw9e1buneTDSkknlHT6InpXZGZmyv9+9uyZ+O6770SzZs3ElStX5O1hYWGic+fOwsDAQHzzzTcqf11n7wGKj48vvoaTzrlw4YKws7MTkiSJsWPHquyLjo4WY8eOFZIkiRYtWoiHDx+q7E9NTRVCCDFjxgzRoEEDIUmSaNu2LXt8SKdwzg/Rf7Lm5MyZMwfBwcFIT09H48aNUatWLXlyc9OmTTF9+nRIkoT58+cDAIYPHw57e3vo6+vLc4VMTU1L8lKolKtXrx6Cg4PRo0cPLFq0CMCrtXyAVz0748ePR2ZmJpYsWYKePXvi559/hr29PTIzM+VHWISGhqJq1aro378/OnbsyB4f0ikMP6Tzsi/l/+LFCzx9+hQhISGQJEkeuso+ublhw4bw9/cHAMyfPx/6+voYPHgwHBwcOEmUikxkZCTS09NRtWpVAK8C0I4dO9CzZ89cA9DEiRMBQA5AgYGB8gNMd+3ahXv37mHWrFnyZH0inVLSXU9EJSn7UNeSJUvEL7/8IiIjI8WsWbOEJEnCzs5O7N+/Xy6TfWLp2bNnRffu3YUkSWLmzJkq+4gK0927d0W5cuVEly5d5GGsrEnK58+fFzVr1sxzCGzMmDHCyMhIODs7i+HDh4vhw4eLSpUqiUqVKonbt28X+7UQvQsYfoiEEMuWLROSJIkhQ4aI58+fi2fPnolp06YJSZLEBx98IE6cOCGXzR5yTp48Kfr16ycuX75cEs0mHXHjxg3RsWNH0bFjRxEXF5dj/5sC0LNnz8SiRYtE/fr1hSRJwsjISNSsWVNcvHixuJpP9M7hIoekk7IPdaWnp8PLywsVK1bE5MmT5WdvxcTEYNGiRZgzZw58fHwwefJkNG3aNEf9lJQUzpegIvfw4UOYmJhAqVRi7969cHJyQo0aNeT9Fy9eRM+ePXHt2jWMHj0aCxculPelpqbixYsX2L9/PypUqIBq1aqhQoUKJXEZRO8EzvkhnZQVXDZs2ICyZcsiKipKJfgAgIWFBcaOHQshBAICAgBADkBZt7fr6+sz+FCREv/dgm5vbw8A+PPPP9GpUyd8+umnmDhxIpycnACozgFavHgxAMgBSE9PD+bm5ujRo0fJXATRO4bhh3TW/v37MWDAANSpUwcvX75ExYoVAaiu7mxubo6vv/4aABAQEAADAwOMHTsWzZs3lwMUUVF6fWVwJycn9O/fHxs3boS+vj7GjRunEoB+/vln9OrVSyUAGRgYqPRWEuk6hh/SWW3atMGwYcPwww8/AACuXr0qL/WfXVYA0tfXx6xZs1CmTBk0atQIZcqUKYlmk46rVasWJk2aBH19faxZswYAVAKQi4uLSgBKSkrCihUrGHyIsuGcH9JJ6enpMDB4lf0nTpyIefPmwcbGBr/99hvc3d1zrfP8+XOsWLECH330EVdvpiIncnkeXPZeyRs3bmDevHlYt24dPvvsM5UABLyaA9SmTRskJCQgIiICNjY2xdp+oncZww+Vem960nWW8ePHY8GCBXB3d8f333+PRo0a5Vout19IRIUt+2c2Li4OMTExsLS0hIGBAYyNjeVybwtAV65cgZGRkco2ImL4oVIu+y+REydO4P79+7hy5Qrc3d3h7OyMKlWqyGVHjx6NpUuXvjUAERWl7J/ZlStX4qeffsLJkydhY2ODFi1aYObMmSo9j28LQESUi5K5w56o6GUtAieEEPPmzRMVKlQQBgYGQpIkoa+vL5ydnUVwcLBKna+++kpIkiSaNWsmzpw5U9xNJh10+/Ztcf/+fSGE6qKb06dPF5IkCRcXF/HVV18JPz8/oa+vL2xsbHJ8Nq9fvy4GDRokFAqF8PX15eKFRG/B8EOl3ty5c4UkSeLDDz8U69evF+vXrxcff/yxkCRJ6OnpiW3btqmUHz16tJAkSdSqVUucO3euhFpNuuDo0aNCkiSxYMECkZKSIm9fuXKlKFeunBg0aJDKZ9DR0VFIkiTKlSuXIwDduHFD9OrVS1haWopHjx4V2zUQaSOGHyrVTp8+LaytrUWPHj3EjRs3VPZ9++23wtTUVOjp6Yl///1XZd+QIUOEJEkiIiKiGFtLuuTq1auiYsWKokmTJuLvv/+Wt1+7dk24urqKLl26iLNnzwohhHj58qVo2rSpUCqVomfPnkKSJGFqairCw8NVjnnr1q0cT3Enopz4FEYq1SIiIvDs2TP06tUL1atXhxACaWlpAICRI0di6tSpEEJg3LhxePLkCTIyMgC8mmvx6NEjODo6lmDrqTQS/02z3LZtG+Li4jB+/Hi0a9cOwKv5O5Ik4cKFC+jTpw8aNmyIzMxMvP/++7h+/TqWLl2Kbdu2YeTIkUhMTETLli1x5swZ+dhcuZkofxh+qFR78uQJAKjcIWNoaIjMzEwAwNixY9GhQwfcvHkTCQkJ8srNAGBra1v8DaZSL+tuwczMTCQlJaF69eoAXi250LlzZ8TFxSEkJAR9+vQBAAwbNgznz5/HlClT0K1bN+jp6aF///6wtrYGADRu3Bjnzp0rmYsh0lIMP1QqiGw3LWb/t5WVFQBg586diImJkX/x6OnpITU1FQDQoEEDPHv2DBcuXAAALgZHxaJ27dowMjJC3759MXjwYMybNw/e3t6wt7dHs2bNAACRkZEIDg6Gh4cHBg0aBDMzMwCvHr0CAO3bt4eVlRUfsUJUQFzhmbTe6+v4pKWlwcjICADQtWtXeHh44Pfff0enTp3QqVMnean/rDLR0dGwsbHhwoVUrPr06YPbt29j8uTJuHr1Knr37o2AgACYmprKZR4/foyIiAgMGjQISqVS3r5161aUL18eGzduRFpamhyKiCh/2PNDWi178Pnpp5/g6+sLDw8PLF68GGfPnoWRkRGGDh2KjIwMjBo1Cj///DMeP34s9+789ddfCAoKQt26dVG+fPmSvBTSIVm9k9l7KU+cOIGoqCgAr1YgBwATExOYmppiw4YNcs/kr7/+iu3bt8PW1hYZGRkMPkRq4CKHVCrMmjULU6dOhampKdLS0pCSkoKmTZti+vTpaNeuHRYtWoS5c+ciLS0NTZs2RdeuXXHp0iUEBQXh+fPnCA0NRe3atUv6MkiHxMfHY9iwYbCzs0NcXBzWrl2LGjVqYPv27WjQoIFcbuzYsVi8eDHMzMzg6OiIa9euwdTUFCEhIfzMEqmJ4Ye0UvYen9DQUHTp0gU+Pj748ssvoa+vj71792Ly5MlwcnLCsmXL0L59e+zcuROrV69GUFAQAKBMmTJo0qQJfvzxR/4SoRIRGxsLQ0NDmJiY4Ouvv8aiRYtQo0YN7NixAy4uLnK5gIAA7NmzB8nJyahVqxb8/f1Ro0aNEmw5kXZj+CGt9ujRI/zzzz+YO3cutm3bhrp168r7NmzYgE8//RTVqlXD0qVL8cEHHwAAjhw5gtTUVFSoUAF2dnYwNzcvodYTqT4v7k0B6Pnz5zAyMoK+vj7Kli1bUs0lKhUYfkhrzZs3Dxs3bkTlypVhbm6OLVu2qDytHQA2btyIgQMHonr16pg5cyZ69uxZgi0myl32nszcAlDWj2k+VJeocHDCM2mlzMxM6Ovr4+rVq9i3bx9iYmIAAAYGBvIaPgDg5+eHdevW4ebNm5gxYwa2bNlSUk0mypOenp78uV2wYAHGjBmD69evw9fXF2fPnoUkSQw+RIWIPT+ktdLS0rB+/XqMHj0aaWlpWLdunbww3Ou3v2/evBl+fn5o0qQJ9u/fr3I7MdG7IvvndsKECZg/fz6aNGmCw4cPw9DQkAGIqJBwnR/SWoaGhvjkk0+QkZGBr776CosXL4alpSXatWsn/yWd9YukX79+MDAwQP369Rl86J2V/XM7d+5cKBQK9OrVS16TiogKB3t+SOulpaVh9erVGD16NFxdXeXb24GcPUBE2oCfW6KixZ4f0nqGhoYYPHgwAGD06NGYNm0aAOTaA0SkDfh5JSpaDD9UKrwegGbOnInU1FT4+PjwFwkREangsBeVKmlpaVi7di2GDRuGNm3aYNeuXTAxMSnpZhER0TuEPT9UqhgaGmLQoEEwNDREixYtGHyIiCgH9vwQERGRTuFkCCIiItIpDD9ERESkUxh+iIiISKcw/BAREZFOYfghIiIincLwQ0RERDqF4YeIiIh0CsMPERER6RSGH6IiIEmSyktPTw/m5uZ47733sGbNGpT02qIDBgyAJEk4dOiQynZHR0dIklQyjVJTYGAgJEmCv79/vutIkgRHR0eNz+3v7w9JkhAYGKjxsd5EnWskorzx8RZERah///4AgIyMDNy6dQtHjhxBaGgoDhw4gK1bt5Zw64qGl5cXQkJCEBERUSgBg4iosDH8EBWh13sEgoOD8cEHH2Dbtm3o168ffHx8SqZheThw4ADS0tJKuhlEREWKw15Exaht27bw8/MDAOzevbtkG5MLJycn1KpVq6SbQURUpBh+iIqZq6srACAyMlLeljUHJTU1FTNmzECtWrWgUCjQtWtXuUxiYiJmzJgBFxcXGBsbw8zMDJ6enm8MUTt37kTTpk1RtmxZ2Nra4pNPPsHDhw/zLP+mOT/37t3DF198AWdnZ5QpUwZWVlZo2rQp5syZg+TkZNy5cweSJCEkJAQAULVqVZV5T9kJIbB+/Xq0atUK5ubmKFu2LOrXr4+FCxfm2fN0/vx5+Pj4QKlUQqlUom3btjh27Fie11JQQghs3boVffr0QY0aNWBiYgJTU1M0bdoUP/zwAzIzM99YPywsDO3bt4e5uTnMzMzQtm1bHD9+PM/yFy5cQL9+/VCxYkUoFArY29tj4MCBuHPnTqFdExHljsNeRMUsISEBAKBQKFS2Z2ZmomvXrvj333/h6emJ+vXrw8rKCgDw+PFjvP/++7h8+TIqVqyItm3b4sWLFzh27Bg++ugjBAQEYMKECSrHW758OUaMGAF9fX14enrC2toa+/fvR7NmzdCgQYMCtfnff/9F586dERcXh2rVqqFLly5ISkrC5cuXMWnSJPj6+qJcuXLo378/9u3bh8ePH6N79+4oV65cjmNlZmaiT58+2LFjB8zMzNCkSROUK1cOYWFh+Prrr3Hw4EH88ccf0NP7399mYWFheP/99/HixQs0bNgQtWrVwsWLF+Hp6YkBAwYU6FrykpKSAl9fX1hYWKBOnTpo1KgRnj59imPHjmH48OE4ceJEnhObjx49iqFDh6J69ero2LEjbt68if379+Pff//Fnj170LZtW5XyO3fuhK+vL1JTU+Hm5obmzZvj1q1bCAwMxB9//IGQkBDUrVu3UK6LiHIhiKjQARC5/e+VmZkpPDw8BAAxadKkHOWrV68u7t+/n6Nex44dBQAxbtw4kZqaKm+/deuWcHJyEvr6+uLcuXPy9oiICKFQKIRCoRAHDx6UtyclJYm2bdvK58u+TwghqlSpkqPdz58/F+XLlxcAxJIlS0RmZqbK/pCQEBEbGyt/7enpKQCIiIiIXN+befPmCQCibdu24smTJ/L2xMRE0alTJwFALF++XN6ekZEhatWqJQCIgIAAlWNNnjxZvpZp06bler7cABBVqlRR2ZaWliZ27twpUlJSVLY/efJENG7cWAAQISEhKvumTZsmn/+bb75ReW9++OEHAUDY29uL5ORkefvt27eFsbGxUCqVOY63fv16AUA0adJEZfu6desKfI1ElDeGH6Ii8Hr4SU9PF9evXxcDBgwQAIRCoRA3b97MUX7Hjh05jnX27FkBQDRv3jxH8BBCiN27dwsAYsSIEfK2KVOmCABi8ODBOcpfvXpVSJKU7/CTFVZ8fHzyde1vCj9paWnC2tpamJqaiujo6Bz7o6KihEKhEC4uLvK2AwcOCACiRo0aOa4/LS1NVK5cuVDCz5sEBwcLAGL06NEq27PCT5UqVURaWlqOeu7u7gKA2LJli7ztyy+/FADEypUrcz1X165dBQBx+vRpeRvDD1Hh4rAXURHKbf6Mqakp1q9fDycnpxxlO3XqlKN8cHAwAKBLly65Hq9ly5YAgJMnT8rbQkNDAQC9evXKUb5mzZpwdXXFmTNn8nUN+/fvBwAMHTo0X+Xf5OzZs3j69Ck6duwIa2vrHPttbW3h7OyMixcvIjk5GWXLlpWvpWfPnjmu38DAAD169MDixYs1bluW8PBwBAUF4e7du3jx4gWEEPJQ5Y0bN3Kt0717dxgY5Pxx2rdvX4SFhSE0NBR9+/YFoPr9zE3Lli2xe/dunDx5Eo0aNSqMSyKi1zD8EBWhrHV+9PT0YGZmBhcXF3Tr1g0WFhY5ytrY2OSYBwRAngA7fvx4jB8/Ps9zPX36VP531qTmypUr51q2cuXK+Q4/WROzXw9r6si6lr179751McXnz5+jYsWK+bqWwpCamooBAwa8cf2lrBD0uipVquS6PWudo+yTzLPeAzs7uze2J/v3k4gKF8MPUREqyMq/ZcqUyXV7RkYGAOC9995DtWrV8qyfvSdF/LeCdGGu1lwYx8q6FmdnZzRv3vyNZbOCYFFcS24WL16MrVu3ol69eliwYAEaNWoECwsLGBoa4vr166hZs2aBV+bOrXxGRgYkScInn3zyxrqc8ExUdBh+iN5xlSpVAgD06NEDI0eOzFcde3t7XL9+HXfv3oWzs3OO/ffu3cv3+R0cHHD16lXcvHlT4zWAsq6lXr16+Q6G9vb2AIC7d+/mur8g1/Imu3btAgA5AGV3+/btN9Z9W9uyrgF49R7cunULy5Ytg5mZmSZNJiI1cZ0fondcmzZtABRsUcSseUA7duzIse/69esIDw8v8PlXrVqVr/JGRkYAgPT09Bz7mjRpAqVSiYMHDyI+Pj5fx8u6lp07d+boSUlPT8fOnTvzdZy3iYmJAfAq7L3u559/fmPdnTt3yr1a2W3btg0A0KJFC3mbOt9PIipcDD9E77hmzZrB29sbBw8exFdffYXExESV/ZmZmQgKCpInBgPAwIEDYWRkhA0bNuDw4cPy9uTkZHz55ZdvXbAvu88++wzW1tb4448/sHz58hwB5PDhw4iLi5O/zurluHbtWo5jKRQKjB07FrGxsejevXuuPSbnz5/H9u3b5a9bt26NGjVq4OrVq1i4cKFK2VmzZuXZ61JQNWrUAAD8+OOPKtt/+eUXbNiw4Y117969i+nTp6tsW7VqFY4dOwY7Ozt89NFH8vYxY8agbNmy+Oqrr/DHH3/kONbz58/xww8/IDk5Wd1LIaK3KcE7zYhKLeSxzs+byr/p1uuoqChRv359AUBYWlqK999/X/Tu3Vu0bNlSZQ2e7JYsWSIACH19feHt7S169+4t7O3tRaVKlYSPj0++b3UXQoh//vlHmJqaCgDCyclJ9OrVS/j4+IiqVavmuK19586dAoAwMzMTPXr0EIMGDRKDBg2S92dkZIi+ffvKt/x7eHiI3r17C29vb/l4Xbp0UTn/0aNHRdmyZQUA4erqKvr27StcXFyEoaGh+OyzzwrlVveQkBChr68vAAg3NzfRt29feX2fsWPHCgDC09NTpU7Wre6DBw8WhoaGom7duqJv376iSZMmAoAwNDQUe/fuzXH+nTt3ytdTs2ZN0bVrV9GlSxfRsGFDYWRkJACImJgYuTxvdScqXAw/REWgsMOPEEK8ePFCLF68WLi7uwtTU1OhUCiEo6OjaNeunfj+++9zXTfn559/Fm5ubkKhUAhra2vh6+sr7t+/L/r371+g8CPEqwUVhwwZIqpUqSKMjIyEtbW1cHd3FwEBASqL+AnxKnjVqVNHKBSKPN+LX375RXTo0EFYW1sLQ0NDUaFCBdGsWTPh7+8vrl69mqP82bNnRceOHYWpqakwNTUV77//vggNDVUrGOT1fh87dky8//77wsLCQpiamormzZuLnTt3ioiIiDeGn3Xr1omjR48Kb29vYWpqKsqVKye8vb3FkSNH8mzD9evXxdChQ0W1atWEQqEQSqVS1K5dWwwcOFDs2bNHZU0jhh+iwiUJUcDbF4iIiIi0GOf8EBERkU5h+CEiIiKdwvBDREREOoXhh4iIiHQKww8RERHpFIYfIiIi0ikMP0RERKRTGH6IiIhIpzD8EBERkU5h+CEiIiKdwvBDREREOoXhh4iIiHTK/wOKhZMLWe1SYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "cm_plot_labels = ['normal', 'failure']\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3eca010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_classes=1):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.Conv1=nn.Conv1d(\n",
    "            in_channels=num_features, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.Conv2=nn.Conv1d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.maxpool1=nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.Conv1=nn.Conv1d(\n",
    "            in_channels=128, out_channels=128, kernel_size=5, padding=2\n",
    "        )\n",
    "        self.Conv4=nn.Conv1d(\n",
    "            in_channels=128,out_channels=256, kernel_size=5, padding=2\n",
    "        )\n",
    "        self.maxpool2=nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.relu=nn.ReLu()\n",
    "        self.fc=nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "        self.lstm=nn.LSTM(input_size=256, hidden_size=hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward():\n",
    "        x=x.transposse(1,2)\n",
    "        x=self.relu(self.Conv1(x))\n",
    "        x=self.relu(self.Conv2(x))\n",
    "        x=self.maxpool1(x)\n",
    "        x=self.relu(self.Conv3(x))\n",
    "        x=self.relu(self.Conv4(x))\n",
    "        x=self.maxpool2(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.transpose(1,2)\n",
    "        x=self.lstm(x)\n",
    "        x=self.dropout(x)\n",
    "        x=x[:,-1,:]\n",
    "        x=self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9ff375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def setup_data_loaders(X_train, y_train, X_val, y_val, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6383b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25064, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader = setup_data_loaders(X_train, y_train, X_test, y_test, batch_size)\n",
    "num_features = X_train.shape[1]\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CNNLSTM(num_features, 32).to(device)\n",
    "opti=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss=0.0\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences = sequences.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = model(sequences).squeeze()\n",
    "        loss = nn.MSELoss(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences = sequences.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            outputs = model(sequences).squeeze()\n",
    "            loss = nn.MSELoss(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        # Test the model's performance\n",
    "    test_rmse, _, _, _ = test_model_performance(\n",
    "        model, X_test, engine_test_data, engine_test_unit_indices\n",
    "    )\n",
    "\n",
    "    if test_rmse < best_scores[engine_number][\"RMSE\"]:\n",
    "        best_scores[engine_number][\"RMSE\"] = test_rmse\n",
    "        best_config_per_engine[engine_number].update(\n",
    "            {\n",
    "                \"sequence_length\": sequence_length,\n",
    "                \"num_lstms\": num_lstms,\n",
    "                \"learning_rate\": lr,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"New best model found for engine {engine_number} w/ RMSE: {test_rmse}\"\n",
    "        )\n",
    "        torch.save(model.state_dict(), f\"best_saved_models/best_model_engine_{engine_number}_{round(test_rmse,2)}.pt\")\n",
    "\n",
    "    # Print epoch results\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "        f\"Validation Loss: {val_loss/len(val_loader):.4f}\",\n",
    "        f\"Test RMSE: {test_rmse:.4f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d18a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
